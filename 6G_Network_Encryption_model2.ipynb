{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kram254/6G-Network-Encryption-Machine-Learning-Model/blob/updates/6G_Network_Encryption_model2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS9NURjwhx26"
      },
      "source": [
        "# **Loading Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6RL2CFJj9Dh"
      },
      "source": [
        "## **Mounting google drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2ZWHLpdkf3M",
        "outputId": "c8c5ce0d-4734-4a83-ab5e-605b7518e086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "nsl_kdd_train_path = '/content/drive/MyDrive/6data/nsl-kdd/NSL_KDD_Train.csv'\n",
        "nsl_kdd_test_path = '/content/drive/MyDrive/6data/nsl-kdd/NSL_KDD_Test.csv'\n",
        "nsl_kdd_col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
        "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
        "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
        "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
        "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
        "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
        "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
        "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
        "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
        "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\"]\n",
        "\n",
        "nsl_kdd_train = pd.read_csv(nsl_kdd_train_path, names=nsl_kdd_col_names, skiprows=1)\n",
        "nsl_kdd_test = pd.read_csv(nsl_kdd_test_path, names=nsl_kdd_col_names, skiprows=1)\n",
        "\n",
        "# UNSW-NB15 datasets\n",
        "unsw_files = ['/content/drive/MyDrive/6data/UNSW/UNSW-NB15_1.csv', '/content/drive/MyDrive/6data/UNSW/UNSW-NB15_2.csv',\n",
        "              '/content/drive/MyDrive/6data/UNSW/UNSW-NB15_3.csv', '/content/drive/MyDrive/6data/UNSW/UNSW-NB15_4.csv']\n",
        "unsw_col_names = [\"srcip\", \"sport\", \"dstip\", \"dsport\", \"proto\", \"state\", \"dur\", \"sbytes\", \"dbytes\", \"sttl\", \"dttl\", \"sloss\", \"dloss\", \"service\", \"Sload\", \"Dload\", \"Spkts\",\n",
        "                  \"Dpkts\", \"swin\", \"dwin\", \"stcpb\", \"dtcpb\", \"smeansz\", \"dmeansz\", \"trans_depth\", \"res_bdy_len\", \"Sjit\", \"Djit\", \"Stime\", \"Ltime\", \"Sintpkt\", \"Dintpkt\", \"tcprtt\",\n",
        "                  \"synack\", \"ackdat\", \"is_sm_ips_ports\", \"ct_state_ttl\", \"ct_flw_http_mthd\", \"is_ftp_login\", \"ct_ftp_cmd\", \"ct_srv_src\", \"ct_srv_dst\", \"ct_dst_ltm\", \"ct_src_ltm\",\n",
        "                  \"ct_src_dport_ltm\", \"ct_dst_sport_ltm\", \"ct_dst_src_ltm\", \"attack_cat\", \"Label\"]\n",
        "\n",
        "\n",
        "def read_and_process_csv(file_path, column_names, chunksize=10000):\n",
        "    chunks = []\n",
        "    for chunk in pd.read_csv(file_path, names=column_names, skiprows=1, chunksize=chunksize, low_memory=False):\n",
        "        chunks.append(chunk)\n",
        "    return pd.concat(chunks, ignore_index=True)\n",
        "\n",
        "unsw_frames = [read_and_process_csv(f, unsw_col_names) for f in unsw_files]\n",
        "unsw_nb15 = pd.concat(unsw_frames, ignore_index=True)\n",
        "\n",
        "def convert_mixed_types(df):\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            try:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            except ValueError:\n",
        "                pass\n",
        "    return df\n",
        "\n",
        "unsw_nb15 = convert_mixed_types(unsw_nb15)\n",
        "\n",
        "# Combine datasets\n",
        "combined_data = pd.concat([nsl_kdd_train, nsl_kdd_test, unsw_nb15], ignore_index=True)\n",
        "\n",
        "# Features and labels\n",
        "X = combined_data.drop(columns=['label', 'Label', 'attack_cat', 'srcip', 'dstip'], errors='ignore')\n",
        "y = combined_data[['label', 'Label', 'attack_cat']].fillna('').astype(str).agg('-'.join, axis=1)\n",
        "\n",
        "# Fill missing values\n",
        "for col in X.select_dtypes(include=['float64', 'int64']).columns:\n",
        "    X[col] = X[col].fillna(0)\n",
        "\n",
        "# Encode categorical variables like a boss\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "numeric_features = X.select_dtypes(exclude=['object']).columns\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)])\n",
        "\n",
        "# Encode labels\n",
        "y = pd.factorize(y)[0]\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Models\n",
        "models = {\n",
        "    \"SVM\": SVC(probability=True),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"k-NN\": KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(18, 12))\n",
        "colors = [\"blue\", \"green\", \"red\", \"orange\"]\n",
        "for i, (name, model) in enumerate(models.items()):\n",
        "\n",
        "    # Pipeline\n",
        "    model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                     ('classifier', model)])\n",
        "    # Train the model, like a boss\n",
        "    model_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and compute metrics\n",
        "    y_pred = model_pipeline.predict(X_val)\n",
        "    y_pred_prob = model_pipeline.predict_proba(X_val) if hasattr(model_pipeline, \"predict_proba\") else model_pipeline.decision_function(X_val)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_val, y_pred)\n",
        "    plt.subplot(2, 4, i + 1)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "    plt.title(f\"Confusion Matrix for {name}\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "\n",
        "    # ROC Curve and AUC\n",
        "    if y_val.ndim > 1 and y_val.shape[1] > 2:  # Multi-class\n",
        "        lb = LabelBinarizer()\n",
        "        y_val_bin = lb.fit_transform(np.argmax(y_val, axis=1))\n",
        "        if hasattr(model_pipeline, \"predict_proba\"):\n",
        "            y_score = model_pipeline.predict_proba(X_val)\n",
        "        else:\n",
        "            y_score = model_pipeline.decision_function(X_val)\n",
        "\n",
        "        # Compute ROC curve and AUC for each class\n",
        "        fpr = {}\n",
        "        tpr = {}\n",
        "        roc_auc = {}\n",
        "        for j in range(y_val.shape[1]):\n",
        "            fpr[j], tpr[j], _ = roc_curve(y_val_bin[:, j], y_score[:, j])\n",
        "            roc_auc[j] = auc(fpr[j], tpr[j])\n",
        "\n",
        "        # Plot ROC curve for each class\n",
        "        plt.subplot(2, 4, i + 5)\n",
        "        for j in range(y_val.shape[1]):\n",
        "            plt.plot(fpr[j], tpr[j], label=f'Class {j} (AUC = {roc_auc[j]:.2f})', color=colors[j % len(colors)])\n",
        "        plt.plot([0, 1], [0, 1], \"k--\")\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.title(f\"ROC Curve for {name}\")\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate\")\n",
        "        plt.legend(loc=\"lower right\")\n",
        "    else:  # Binary\n",
        "        fpr, tpr, _ = roc_curve(np.argmax(y_val, axis=1), y_pred_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.subplot(2, 4, i + 5)\n",
        "        plt.plot(fpr, tpr, color=colors[i], lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], \"k--\", lw=2)\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.title(f\"ROC Curve for {name}\")\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate\")\n",
        "        plt.legend(loc=\"lower right\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score\n",
        "# from sklearn.svm import SVC\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler, LabelBinarizer, OneHotEncoder\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.pipeline import Pipeline\n",
        "\n",
        "# # Load NSL-KDD datasets\n",
        "# nsl_kdd_train_path = '/content/drive/MyDrive/6data/nsl-kdd/NSL_KDD_Train.csv'\n",
        "# nsl_kdd_test_path = '/content/drive/MyDrive/6data/nsl-kdd/NSL_KDD_Test.csv'\n",
        "# nsl_kdd_col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
        "#     \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
        "#     \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
        "#     \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
        "#     \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
        "#     \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
        "#     \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
        "#     \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
        "#     \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
        "#     \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\"]\n",
        "\n",
        "# nsl_kdd_train = pd.read_csv(nsl_kdd_train_path, names=nsl_kdd_col_names, skiprows=1)\n",
        "# nsl_kdd_test = pd.read_csv(nsl_kdd_test_path, names=nsl_kdd_col_names, skiprows=1)\n",
        "\n",
        "# # Load UNSW-NB15 datasets\n",
        "# unsw_files = ['/content/drive/MyDrive/6data/UNSW/UNSW-NB15_1.csv', '/content/drive/MyDrive/6data/UNSW/UNSW-NB15_2.csv',\n",
        "#               '//content/drive/MyDrive/6data/UNSW/UNSW-NB15_3.csv', '/content/drive/MyDrive/6data/UNSW/UNSW-NB15_4.csv']\n",
        "# unsw_col_names = [\"srcip\", \"sport\", \"dstip\", \"dsport\", \"proto\", \"state\", \"dur\", \"sbytes\", \"dbytes\", \"sttl\", \"dttl\", \"sloss\", \"dloss\", \"service\", \"Sload\", \"Dload\", \"Spkts\",\n",
        "#                   \"Dpkts\", \"swin\", \"dwin\", \"stcpb\", \"dtcpb\", \"smeansz\", \"dmeansz\", \"trans_depth\", \"res_bdy_len\", \"Sjit\", \"Djit\", \"Stime\", \"Ltime\", \"Sintpkt\", \"Dintpkt\", \"tcprtt\",\n",
        "#                   \"synack\", \"ackdat\", \"is_sm_ips_ports\", \"ct_state_ttl\", \"ct_flw_http_mthd\", \"is_ftp_login\", \"ct_ftp_cmd\", \"ct_srv_src\", \"ct_srv_dst\", \"ct_dst_ltm\", \"ct_src_ltm\",\n",
        "#                   \"ct_src_dport_ltm\", \"ct_dst_sport_ltm\", \"ct_dst_src_ltm\", \"attack_cat\", \"Label\"]\n",
        "# unsw_frames = [pd.read_csv(f, names=unsw_col_names, skiprows=1) for f in unsw_files]\n",
        "# unsw_nb15 = pd.concat(unsw_frames, ignore_index=True)\n",
        "\n",
        "# # Handle mixed types by converting to appropriate types\n",
        "# unsw_nb15['sport'] = pd.to_numeric(unsw_nb15['sport'], errors='coerce')\n",
        "# unsw_nb15['dsport'] = pd.to_numeric(unsw_nb15['dsport'], errors='coerce')\n",
        "\n",
        "# # Combine datasets\n",
        "# combined_data = pd.concat([nsl_kdd_train, nsl_kdd_test, unsw_nb15], ignore_index=True)\n",
        "\n",
        "# # Features and labels (adjust based on actual dataset column names)\n",
        "# X = combined_data.drop(columns=['label', 'Label', 'attack_cat', 'srcip', 'dstip'], errors='ignore')\n",
        "# y = combined_data[['label', 'Label', 'attack_cat']].fillna('').astype(str).agg('-'.join, axis=1)\n",
        "\n",
        "# # Fill missing values in numerical columns\n",
        "# for col in X.select_dtypes(include=['float64', 'int64']).columns:\n",
        "#     X[col] = X[col].fillna(0)\n",
        "\n",
        "# # Encode categorical variables and standardize numerical features\n",
        "# categorical_features = X.select_dtypes(include=['object']).columns\n",
        "# numeric_features = X.select_dtypes(exclude=['object']).columns\n",
        "\n",
        "# preprocessor = ColumnTransformer(\n",
        "#     transformers=[\n",
        "#         ('num', StandardScaler(), numeric_features),\n",
        "#         ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)])\n",
        "\n",
        "# # Encode labels\n",
        "# lb = LabelBinarizer()\n",
        "# y = lb.fit_transform(y)\n",
        "\n",
        "# # Split data\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Initialize models\n",
        "# models = {\n",
        "#     \"SVM\": SVC(probability=True),\n",
        "#     \"Random Forest\": RandomForestClassifier(),\n",
        "#     \"Decision Tree\": DecisionTreeClassifier(),\n",
        "#     \"k-NN\": KNeighborsClassifier()\n",
        "# }\n",
        "\n",
        "# # Plotting setup\n",
        "# plt.figure(figsize=(18, 12))\n",
        "\n",
        "# # Colors for ROC curves\n",
        "# colors = [\"blue\", \"green\", \"red\", \"orange\"]\n",
        "\n",
        "# for i, (name, model) in enumerate(models.items()):\n",
        "#     # Create pipeline for each model\n",
        "#     model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "#                                      ('classifier', model)])\n",
        "#     # Train the model\n",
        "#     model_pipeline.fit(X_train, np.argmax(y_train, axis=1))\n",
        "\n",
        "#     # Predict and compute metrics\n",
        "#     y_pred = model_pipeline.predict(X_val)\n",
        "#     y_pred_prob = model_pipeline.predict_proba(X_val) if hasattr(model_pipeline, \"predict_proba\") else model_pipeline.decision_function(X_val)\n",
        "\n",
        "#     # Confusion Matrix\n",
        "#     cm = confusion_matrix(np.argmax(y_val, axis=1), y_pred)\n",
        "#     plt.subplot(2, 4, i + 1)\n",
        "#     sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "#     plt.title(f\"Confusion Matrix for {name}\")\n",
        "#     plt.xlabel(\"Predicted\")\n",
        "#     plt.ylabel(\"Actual\")\n",
        "\n",
        "#     # ROC Curve and AUC\n",
        "#     if y_val.shape[1] > 2:  # Multi-class\n",
        "#         lb = LabelBinarizer()\n",
        "#         y_val_bin = lb.fit_transform(np.argmax(y_val, axis=1))\n",
        "#         if hasattr(model_pipeline, \"predict_proba\"):\n",
        "#             y_score = model_pipeline.predict_proba(X_val)\n",
        "#         else:\n",
        "#             y_score = model_pipeline.decision_function(X_val)\n",
        "\n",
        "#         # Compute ROC curve and AUC for each class\n",
        "#         fpr = {}\n",
        "#         tpr = {}\n",
        "#         roc_auc = {}\n",
        "#         for j in range(y_val.shape[1]):\n",
        "#             fpr[j], tpr[j], _ = roc_curve(y_val_bin[:, j], y_score[:, j])\n",
        "#             roc_auc[j] = auc(fpr[j], tpr[j])\n",
        "\n",
        "#         # Plot ROC curve for each class\n",
        "#         plt.subplot(2, 4, i + 5)\n",
        "#         for j in range(y_val.shape[1]):\n",
        "#             plt.plot(fpr[j], tpr[j], label=f'Class {j} (AUC = {roc_auc[j]:.2f})', color=colors[j % len(colors)])\n",
        "#         plt.plot([0, 1], [0, 1], \"k--\")\n",
        "#         plt.xlim([0.0, 1.0])\n",
        "#         plt.ylim([0.0, 1.05])\n",
        "#         plt.title(f\"ROC Curve for {name}\")\n",
        "#         plt.xlabel(\"False Positive Rate\")\n",
        "#         plt.ylabel(\"True Positive Rate\")\n",
        "#         plt.legend(loc=\"lower right\")\n",
        "#     else:  # Binary\n",
        "#         fpr, tpr, _ = roc_curve(np.argmax(y_val, axis=1), y_pred_prob)\n",
        "#         roc_auc = auc(fpr, tpr)\n",
        "#         plt.subplot(2, 4, i + 5)\n",
        "#         plt.plot(fpr, tpr, color=colors[i], lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "#         plt.plot([0, 1], [0, 1], \"k--\", lw=2)\n",
        "#         plt.xlim([0.0, 1.0])\n",
        "#         plt.ylim([0.0, 1.05])\n",
        "#         plt.title(f\"ROC Curve for {name}\")\n",
        "#         plt.xlabel(\"False Positive Rate\")\n",
        "#         plt.ylabel(\"True Positive Rate\")\n",
        "#         plt.legend(loc=\"lower right\")\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "SwOMpYBDUox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAZfqcnIeM7t"
      },
      "source": [
        "**NSL_KDD dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0EjgiSPfhwE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "nsl_kdd_col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
        "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
        "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
        "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
        "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
        "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
        "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
        "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
        "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
        "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\"]\n",
        "\n",
        "nsl_kdd_train = pd.read_csv('/content/drive/MyDrive/6data/nsl-kdd/NSL_KDD_Train.csv', names=nsl_kdd_col_names)\n",
        "nsl_kdd_test = pd.read_csv('/content/drive/MyDrive/6data/nsl-kdd/NSL_KDD_Test.csv', names=nsl_kdd_col_names)\n",
        "\n",
        "nsl_kdd = pd.concat([nsl_kdd_train, nsl_kdd_test], ignore_index=True)\n",
        "\n",
        "print(nsl_kdd.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rgp1mTzTM-Ry"
      },
      "source": [
        "# **UNSW**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjYVCHC9uY-c"
      },
      "outputs": [],
      "source": [
        "unsw_col_names = [\"srcip\", \"sport\", \"dstip\", \"dsport\", \"proto\", \"state\", \"dur\", \"sbytes\", \"dbytes\", \"sttl\", \"dttl\", \"sloss\", \"dloss\", \"service\", \"Sload\", \"Dload\", \"Spkts\",\n",
        "                  \"Dpkts\", \"swin\", \"dwin\", \"stcpb\", \"dtcpb\", \"smeansz\", \"dmeansz\", \"trans_depth\", \"res_bdy_len\", \"Sjit\", \"Djit\", \"Stime\", \"Ltime\", \"Sintpkt\", \"Dintpkt\", \"tcprtt\",\n",
        "                  \"synack\", \"ackdat\", \"is_sm_ips_ports\", \"ct_state_ttl\", \"ct_flw_http_mthd\", \"is_ftp_login\", \"ct_ftp_cmd\", \"ct_srv_src\", \"ct_srv_dst\", \"ct_dst_ltm\", \"ct_src_ltm\",\n",
        "                  \"ct_src_dport_ltm\", \"ct_dst_sport_ltm\", \"ct_dst_src_ltm\", \"attack_cat\", \"Label\"]\n",
        "\n",
        "unsw_frames = [pd.read_csv(f'/content/drive/MyDrive/6data/UNSW/UNSW-NB15_{i}.csv', names=unsw_col_names) for i in range(1, 5)]\n",
        "\n",
        "unsw_nb15 = pd.concat(unsw_frames, ignore_index=True)\n",
        "\n",
        "print(unsw_nb15.head(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AouVars-NTAn"
      },
      "source": [
        "# **cic_ids17**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPyuCVzlvETN"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import gc\n",
        "\n",
        "\n",
        "# cic_ids17_a_col_names = [\"Destination Port\", \"Flow Duration\", \"Total Fwd Packets\", \"Total Backward Packets\", \"Total Length of Fwd Packets\", \"Total Length of Bwd Packets\",\n",
        "#                          \"Fwd Packet Length Max\", \"Fwd Packet Length Min\", \"Fwd Packet Length Mean\", \"Fwd Packet Length Std\", \"Bwd Packet Length Max\", \"Bwd Packet Length Min\",\n",
        "#                          \"Bwd Packet Length Mean\", \"Bwd Packet Length Std\", \"Flow Bytes/s\", \"Flow Packets/s\", \"Flow IAT Mean\", \"Flow IAT Std\", \"Flow IAT Max\", \"Flow IAT Min\",\n",
        "#                          \"Fwd IAT Total\", \"Fwd IAT Mean\", \"Fwd IAT Std\", \"Fwd IAT Max\", \"Fwd IAT Min\", \"Bwd IAT Total\", \"Bwd IAT Mean\", \"Bwd IAT Std\", \"Bwd IAT Max\",\n",
        "#                          \"Bwd IAT Min\", \"Fwd PSH Flags\", \"Bwd PSH Flags\", \"Fwd URG Flags\", \"Bwd URG Flags\", \"Fwd Header Length\", \"Bwd Header Length\", \"Fwd Packets/s\",\n",
        "#                          \"Bwd Packets/s\", \"Min Packet Length\", \"Max Packet Length\", \"Packet Length Mean\", \"Packet Length Std\", \"Packet Length Variance\", \"FIN Flag Count\",\n",
        "#                          \"SYN Flag Count\", \"RST Flag Count\", \"PSH Flag Count\", \"ACK Flag Count\", \"URG Flag Count\", \"CWE Flag Count\", \"ECE Flag Count\", \"Down/Up Ratio\",\n",
        "#                          \"Average Packet Size\", \"Avg Fwd Segment Size\", \"Avg Bwd Segment Size\", \"Fwd Header Length.1\", \"Fwd Avg Bytes/Bulk\", \"Fwd Avg Packets/Bulk\",\n",
        "#                          \"Fwd Avg Bulk Rate\", \"Bwd Avg Bytes/Bulk\", \"Bwd Avg Packets/Bulk\", \"Bwd Avg Bulk Rate\", \"Subflow Fwd Packets\", \"Subflow Fwd Bytes\",\n",
        "#                          \"Subflow Bwd Packets\", \"Subflow Bwd Bytes\", \"Init_Win_bytes_forward\", \"Init_Win_bytes_backward\", \"act_data_pkt_fwd\", \"min_seg_size_forward\",\n",
        "#                          \"Active Mean\", \"Active Std\", \"Active Max\", \"Active Min\", \"Idle Mean\", \"Idle Std\", \"Idle Max\", \"Idle Min\", \"Label\"]\n",
        "\n",
        "# cic_ids17_a_files = [\"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\", \"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\", \"Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\",\n",
        "#                      \"Friday-WorkingHours-Morning.pcap_ISCX.csv\", \"Monday-WorkingHours.pcap_ISCX.csv\", \"Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\",\n",
        "#                      \"Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\", \"Tuesday-WorkingHours.pcap_ISCX.csv\", \"Wednesday-workingHours.pcap_ISCX.csv\"]\n",
        "\n",
        "# # cic_ids17_a_frames = [pd.read_csv(f'/content/drive/MyDrive/6data/CIC_IDS17/dataA/{file}', names=cic_ids17_a_col_names) for file in cic_ids17_a_files]\n",
        "\n",
        "# def read_and_concat_in_chunks(file_path, chunk_size, concat_df=None):\n",
        "#     for chunk in pd.read_csv(file_path, names=cic_ids17_a_col_names, chunksize=chunk_size):\n",
        "#         concat_df = pd.concat([concat_df, chunk], ignore_index=True) if concat_df is not None else chunk\n",
        "#         gc.collect()  # garbage collector after each chunk to free up memory\n",
        "#     return concat_df\n",
        "\n",
        "# cic_ids17_a = None\n",
        "\n",
        "# for file in cic_ids17_a_files:\n",
        "#     file_path = f'/content/drive/MyDrive/6data/CIC_IDS17/dataA/{file}'\n",
        "#     cic_ids17_a = read_and_concat_in_chunks(file_path, chunk_size=10000, concat_df=cic_ids17_a)\n",
        "\n",
        "# print(cic_ids17_a.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWgWdVVgTIuL"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import gc\n",
        "# import pandas as pd\n",
        "\n",
        "# cic_ids17_b_col_names = [\"Flow ID\", \"Source IP\", \"Source Port\", \"Destination IP\", \"Destination Port\", \"Protocol\", \"Timestamp\", \"Flow Duration\", \"Total Fwd Packets\",\n",
        "#                          \"Total Backward Packets\", \"Total Length of Fwd Packets\", \"Total Length of Bwd Packets\", \"Fwd Packet Length Max\", \"Fwd Packet Length Min\",\n",
        "#                          \"Fwd Packet Length Mean\", \"Fwd Packet Length Std\", \"Bwd Packet Length Max\", \"Bwd Packet Length Min\", \"Bwd Packet Length Mean\", \"Bwd Packet Length Std\",\n",
        "#                          \"Flow Bytes/s\", \"Flow Packets/s\", \"Flow IAT Mean\", \"Flow IAT Std\", \"Flow IAT Max\", \"Flow IAT Min\", \"Fwd IAT Total\", \"Fwd IAT Mean\", \"Fwd IAT Std\",\n",
        "#                          \"Fwd IAT Max\", \"Fwd IAT Min\", \"Bwd IAT Total\", \"Bwd IAT Mean\", \"Bwd IAT Std\", \"Bwd IAT Max\", \"Bwd IAT Min\", \"Fwd PSH Flags\", \"Bwd PSH Flags\",\n",
        "#                          \"Fwd URG Flags\", \"Bwd URG Flags\", \"Fwd Header Length\", \"Bwd Header Length\", \"Fwd Packets/s\", \"Bwd Packets/s\", \"Min Packet Length\", \"Max Packet Length\",\n",
        "#                          \"Packet Length Mean\", \"Packet Length Std\", \"Packet Length Variance\", \"FIN Flag Count\", \"SYN Flag Count\", \"RST Flag Count\", \"PSH Flag Count\",\n",
        "#                          \"ACK Flag Count\", \"URG Flag Count\", \"CWE Flag Count\", \"ECE Flag Count\", \"Down/Up Ratio\", \"Average Packet Size\", \"Avg Fwd Segment Size\",\n",
        "#                          \"Avg Bwd Segment Size\", \"Fwd Header Length.1\", \"Fwd Avg Bytes/Bulk\", \"Fwd Avg Packets/Bulk\", \"Fwd Avg Bulk Rate\", \"Bwd Avg Bytes/Bulk\",\n",
        "#                          \"Bwd Avg Packets/Bulk\", \"Bwd Avg Bulk Rate\", \"Subflow Fwd Packets\", \"Subflow Fwd Bytes\", \"Subflow Bwd Packets\", \"Subflow Bwd Bytes\",\n",
        "#                          \"Init_Win_bytes_forward\", \"Init_Win_bytes_backward\", \"act_data_pkt_fwd\", \"min_seg_size_forward\", \"Active Mean\", \"Active Std\", \"Active Max\",\n",
        "#                          \"Active Min\", \"Idle Mean\", \"Idle Std\", \"Idle Max\", \"Idle Min\", \"Label\"]\n",
        "\n",
        "# cic_ids17_b_files = [\"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\", \"Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\", \"Friday-WorkingHours-Morning.pcap_ISCX.csv\",\n",
        "#                      \"Monday-WorkingHours.pcap_ISCX.csv\", \"Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\"\n",
        "#                     #  \"Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\", \"Tuesday-WorkingHours.pcap_ISCX.csv\", \"Wednesday-workingHours.pcap_ISCX.csv\"\n",
        "#                      ]\n",
        "\n",
        "\n",
        "# processed_data_dir = '/content/drive/MyDrive/6data/CIC_IDS17/dataB/processed/'\n",
        "# if not os.path.exists(processed_data_dir):\n",
        "#     os.makedirs(processed_data_dir)\n",
        "\n",
        "# def process_and_save_chunks(file_path, chunk_size, processed_data_dir):\n",
        "#     for i, chunk in enumerate(pd.read_csv(file_path, names=cic_ids17_b_col_names, chunksize=chunk_size, encoding='ISO-8859-1')):\n",
        "#         # Perform preprocessing on the chunk\n",
        "\n",
        "#         chunk.to_csv(f'{processed_data_dir}processed_chunk_{i}.csv', index=False)\n",
        "#         gc.collect()  # garbage collector after each chunk to free up memory\n",
        "\n",
        "# for file in cic_ids17_b_files:\n",
        "#     file_path = f'/content/drive/MyDrive/6data/CIC_IDS17/dataB/{file}'\n",
        "#     process_and_save_chunks(file_path, chunk_size=10000, processed_data_dir=processed_data_dir)\n",
        "\n",
        "# print(\"All files have been processed and saved in chunks.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCWsnekOH4Oq"
      },
      "outputs": [],
      "source": [
        "# processed_files = [os.path.join(processed_data_dir, f) for f in os.listdir(processed_data_dir) if f.startswith('processed_chunk_')]\n",
        "\n",
        "# # Ensure there are processed files\n",
        "# if processed_files:\n",
        "#     # Option 1: Read the head of the first file\n",
        "#     first_chunk = pd.read_csv(processed_files[0])\n",
        "#     print(\"First 10 rows of the first processed chunk:\")\n",
        "#     print(first_chunk.head(10))\n",
        "\n",
        "#     # Option 2: If you want to read the first 10 rows from each file and print them\n",
        "#     # for file in processed_files:\n",
        "#     #     chunk = pd.read_csv(file)\n",
        "#     #     print(f\"First 10 rows of {os.path.basename(file)}:\")\n",
        "#     #     print(chunk.head(10))\n",
        "#     #     print(\"\\n-------------------------------------------------------\\n\")\n",
        "\n",
        "# else:\n",
        "#     print(\"No processed chunk files found in the specified directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7M3cIpaInN1"
      },
      "source": [
        "# **Describe() and Info() the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQNuXPJ5nWul"
      },
      "outputs": [],
      "source": [
        "# print(\"NSL-KDD Dataset:\")\n",
        "# print(nsl_kdd.info())\n",
        "# print(nsl_kdd.describe())\n",
        "\n",
        "# print(\"\\nUNSW-NB15 Dataset:\")\n",
        "# print(unsw_nb15.info())\n",
        "# print(unsw_nb15.describe())\n",
        "\n",
        "# processed_data_dir = '/content/drive/MyDrive/6data/CIC_IDS17/dataB/processed/'\n",
        "\n",
        "# processed_files = [os.path.join(processed_data_dir, f) for f in os.listdir(processed_data_dir) if f.startswith('processed_chunk_')]\n",
        "\n",
        "# if processed_files:\n",
        "#     first_chunk = pd.read_csv(processed_files[0])\n",
        "\n",
        "#     print(\"Info of the first processed chunk:\")\n",
        "#     first_chunk.info()\n",
        "\n",
        "#     print(\"\\nDescription of the first processed chunk:\")\n",
        "#     print(first_chunk.describe())\n",
        "\n",
        "# else:\n",
        "#     print(\"No processed chunk files found in the specified directory.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS4e0O3mU-hA"
      },
      "source": [
        "# **Heatmap of Correlation Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hz8eZL1HVCU2"
      },
      "outputs": [],
      "source": [
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.figure(figsize=(10, 8))\n",
        "# correlation_matrix = nsl_kdd.corr()\n",
        "# sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\")\n",
        "# plt.title('NSL-KDD Correlation Matrix')\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4tdykdxVRzl"
      },
      "source": [
        "# **Boxplots for Feature Distribution Across Classes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y69l81aVQRB"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.boxplot(x='label', y='src_bytes', data=nsl_kdd)\n",
        "plt.title('Distribution of src_bytes Across Classes in NSL-KDD')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhUsLJBTViMc"
      },
      "source": [
        "# **Feature Importance from Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaTCfhsJVi6M"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "nsl_kdd_col_names = [\n",
        "    \"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\",\n",
        "    \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
        "    \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
        "    \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
        "    \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"label\"\n",
        "]\n",
        "\n",
        "X = nsl_kdd.drop('label', axis=1)\n",
        "y = nsl_kdd['label']\n",
        "\n",
        "X = pd.get_dummies(X)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "rf.fit(X, y)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sorted_idx = rf.feature_importances_.argsort()\n",
        "plt.barh(X.columns[sorted_idx], rf.feature_importances_[sorted_idx])\n",
        "plt.xlabel(\"Random Forest Feature Importance\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.title(\"Feature Importance in NSL-KDD Dataset\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWMIVcUw4_XW"
      },
      "source": [
        "# **Milestone 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpjNOxeU5FiQ"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN2aFmFcKDc_"
      },
      "source": [
        "#### Define model architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZhUS8j9KC4d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the model architectures\n",
        "def build_dolev_yao_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=input_shape),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(2, activation='softmax')  # Assuming binary classification\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_canetti_krawczyk_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=input_shape),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(2, activation='softmax')  # Assuming binary classification\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sulzAAvNfcw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj9qz9XIQsQx"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Dropout, BatchNormalization\n",
        "# from keras.callbacks import EarlyStopping\n",
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "# from keras.utils import to_categorical\n",
        "# import gc\n",
        "# from google.colab import drive\n",
        "# import keras_tuner as kt\n",
        "\n",
        "# def print_metrics(y_true, y_pred):\n",
        "#     accuracy = accuracy_score(y_true, y_pred)\n",
        "#     precision = precision_score(y_true, y_pred, average='weighted')\n",
        "#     recall = recall_score(y_true, y_pred, average='weighted')\n",
        "#     f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "#     confusion_mat = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "#     print(f\"Accuracy: {accuracy:.4f}\")\n",
        "#     print(f\"Precision: {precision:.4f}\")\n",
        "#     print(f\"Recall: {recall:.4f}\")\n",
        "#     print(f\"F1-Score: {f1:.4f}\")\n",
        "#     print(\"Confusion Matrix:\")\n",
        "#     print(confusion_mat)\n",
        "#     print()\n",
        "\n",
        "# # Function to process data from the provided dataset\n",
        "# def process_data(dataset, feature_cols, label_col):\n",
        "#     X = pd.get_dummies(dataset[feature_cols])\n",
        "#     y = dataset[label_col]\n",
        "\n",
        "#     # Encode string labels into integers\n",
        "#     label_encoder = LabelEncoder()\n",
        "#     y = label_encoder.fit_transform(y)\n",
        "\n",
        "#     num_classes = len(label_encoder.classes_)\n",
        "\n",
        "#     # Convert to appropriate data types\n",
        "#     X = X.astype('float32')\n",
        "#     y = to_categorical(y, num_classes=num_classes).astype('float32')\n",
        "\n",
        "#     return X, y, num_classes\n",
        "\n",
        "# # Train and fine-tune on each dataset\n",
        "# datasets = [nsl_kdd, unsw_nb15]  # Assuming nsl_kdd and unsw_nb15 are defined somewhere\n",
        "# label_cols = ['label', 'Label']\n",
        "\n",
        "# unsw_col_names = [\"srcip\", \"sport\", \"dstip\", \"dsport\", \"proto\", \"state\", \"dur\", \"sbytes\", \"dbytes\", \"sttl\", \"dttl\", \"sloss\", \"dloss\", \"service\", \"Sload\", \"Dload\", \"Spkts\",\n",
        "#                   \"Dpkts\", \"swin\", \"dwin\", \"stcpb\", \"dtcpb\", \"smeansz\", \"dmeansz\", \"trans_depth\", \"res_bdy_len\", \"Sjit\", \"Djit\", \"Stime\", \"Ltime\", \"Sintpkt\", \"Dintpkt\", \"tcprtt\",\n",
        "#                   \"synack\", \"ackdat\", \"is_sm_ips_ports\", \"ct_state_ttl\", \"ct_flw_http_mthd\", \"is_ftp_login\", \"ct_ftp_cmd\", \"ct_srv_src\", \"ct_srv_dst\", \"ct_dst_ltm\", \"ct_src_ltm\",\n",
        "#                   \"ct_src_dport_ltm\", \"ct_dst_sport_ltm\", \"ct_dst_src_ltm\", \"attack_cat\"]\n",
        "\n",
        "# nsl_kdd_col_names = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\",\n",
        "#                      \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
        "#                      \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
        "#                      \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
        "#                      \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\"]\n",
        "\n",
        "# feature_cols = [nsl_kdd_col_names, unsw_col_names[:-1]]  # Exclude the 'Label' column\n",
        "\n",
        "# for dataset, label_col, feature_cols in zip(datasets, label_cols, feature_cols):\n",
        "#     print(f\"Training on dataset: {label_col}\")\n",
        "\n",
        "#     X, y, num_classes = process_data(dataset, feature_cols, label_col)\n",
        "\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "#     X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
        "\n",
        "#     # Define the model architecture\n",
        "#     def build_model(num_classes, hp):\n",
        "#         model = Sequential([\n",
        "#             Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "#             BatchNormalization(),\n",
        "#             Dropout(0.2),\n",
        "#             Dense(64, activation='relu'),\n",
        "#             BatchNormalization(),\n",
        "#             Dropout(0.2),\n",
        "#             Dense(num_classes, activation='softmax')\n",
        "#         ])\n",
        "#         model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#         return model\n",
        "\n",
        "\n",
        "\n",
        "#         model = build_model(num_classes)\n",
        "\n",
        "#         early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "#         model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=120, batch_size=64, callbacks=[early_stopping])\n",
        "\n",
        "#         # Evaluate on test set\n",
        "#         y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "#         y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "#         print(f\"Metrics for {label_col} dataset:\")\n",
        "#         print_metrics(y_true, y_pred)\n",
        "\n",
        "#         # Free up memory\n",
        "#         del X, y, X_train, y_train, X_val, y_val, X_test, y_test, model\n",
        "#         gc.collect()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     # Define the hyperparameter search space\n",
        "#     tuner = kt.RandomSearch(\n",
        "#         lambda hp: build_model(num_classes, hp),\n",
        "#         objective='val_loss',\n",
        "#         max_trials=10,\n",
        "#         directory='my_tuning_directory',\n",
        "#         project_name='my_tuning_project'\n",
        "#     )\n",
        "\n",
        "#     # Perform hyperparameter tuning\n",
        "#     tuner.search_space = {\n",
        "#         'batch_size': kt.IntHyperParameter(32, 128, step=32),\n",
        "#         'epochs': kt.IntHyperParameter(10, 30, step=5),\n",
        "#         'learning_rate': kt.RealHyperParameter(0.001, 0.1, step=0.01)\n",
        "#     }\n",
        "\n",
        "#     tuner.run_trial()\n",
        "\n",
        "#     # Evaluate on test set\n",
        "#     best_model = tuner.get_best_models(num_models=1)[0]\n",
        "#     y_pred = np.argmax(best_model.predict(X_test), axis=1)\n",
        "#     y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "#     print(f\"Metrics for {label_col} dataset:\")\n",
        "#     print_metrics(y_true, y_pred)\n",
        "\n",
        "#     # Free up memory\n",
        "#     del X, y, X_train, y_train, X_val, y_val, X_test, y_test\n",
        "#     gc.collect()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Dropout\n",
        "# from keras.callbacks import EarlyStopping\n",
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "# from keras.utils import to_categorical\n",
        "# import gc\n",
        "# from google.colab import drive\n",
        "\n",
        "# def print_metrics(y_true, y_pred):\n",
        "#     accuracy = accuracy_score(y_true, y_pred)\n",
        "#     precision = precision_score(y_true, y_pred, average='weighted')\n",
        "#     recall = recall_score(y_true, y_pred, average='weighted')\n",
        "#     f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "#     confusion_mat = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "#     print(f\"Accuracy: {accuracy:.4f}\")\n",
        "#     print(f\"Precision: {precision:.4f}\")\n",
        "#     print(f\"Recall: {recall:.4f}\")\n",
        "#     print(f\"F1-Score: {f1:.4f}\")\n",
        "#     print(\"Confusion Matrix:\")\n",
        "#     print(confusion_mat)\n",
        "#     print()\n",
        "\n",
        "\n",
        "# # Function to process data from the provided dataset\n",
        "# def process_data(dataset, feature_cols, label_col):\n",
        "#     X = pd.get_dummies(dataset[feature_cols])\n",
        "#     y = dataset[label_col]\n",
        "\n",
        "#     # Encode string labels into integers\n",
        "#     label_encoder = LabelEncoder()\n",
        "#     y = label_encoder.fit_transform(y)\n",
        "\n",
        "#     num_classes = len(label_encoder.classes_)\n",
        "\n",
        "#     # Convert to appropriate data types\n",
        "#     X = X.astype('float32')\n",
        "#     y = to_categorical(y, num_classes=num_classes).astype('float32')\n",
        "\n",
        "#     return X, y, num_classes\n",
        "\n",
        "\n",
        "# # Train and fine-tune on each dataset\n",
        "# datasets = [nsl_kdd, unsw_nb15]  # Assuming nsl_kdd and unsw_nb15 are defined somewhere\n",
        "# label_cols = ['label', 'Label']\n",
        "# unsw_col_names = [\"srcip\", \"sport\", \"dstip\", \"dsport\", \"proto\", \"state\", \"dur\", \"sbytes\", \"dbytes\", \"sttl\", \"dttl\", \"sloss\", \"dloss\", \"service\", \"Sload\", \"Dload\", \"Spkts\",\n",
        "#                   \"Dpkts\", \"swin\", \"dwin\", \"stcpb\", \"dtcpb\", \"smeansz\", \"dmeansz\", \"trans_depth\", \"res_bdy_len\", \"Sjit\", \"Djit\", \"Stime\", \"Ltime\", \"Sintpkt\", \"Dintpkt\", \"tcprtt\",\n",
        "#                   \"synack\", \"ackdat\", \"is_sm_ips_ports\", \"ct_state_ttl\", \"ct_flw_http_mthd\", \"is_ftp_login\", \"ct_ftp_cmd\", \"ct_srv_src\", \"ct_srv_dst\", \"ct_dst_ltm\", \"ct_src_ltm\",\n",
        "#                   \"ct_src_dport_ltm\", \"ct_dst_sport_ltm\", \"ct_dst_src_ltm\", \"attack_cat\"]\n",
        "# nsl_kdd_col_names = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\",\n",
        "#                      \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
        "#                      \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
        "#                      \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
        "#                      \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# feature_cols = [nsl_kdd_col_names, unsw_col_names[:-1]]\n",
        "\n",
        "\n",
        "# X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import keras_tuner as kt\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Dropout, BatchNormalization\n",
        "\n",
        "# num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# # Define the model architecture\n",
        "# def build_model(hp):\n",
        "#     # Define the model architecture\n",
        "#     model = Sequential([\n",
        "#         Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "#         BatchNormalization(),\n",
        "#         Dropout(0.2),\n",
        "#         Dense(64, activation='relu'),\n",
        "#         BatchNormalization(),\n",
        "#         Dropout(0.2),\n",
        "#         Dense(num_classes, activation='softmax')\n",
        "#     ])\n",
        "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# # Define the hyperparameter search space\n",
        "# tuner = kt.Hyperband(\n",
        "#     build_model,\n",
        "#     objective='val_loss',\n",
        "#     max_epochs=10,\n",
        "#     directory='my_tuning_directory',\n",
        "#     project_name='my_tuning_project'\n",
        "# )\n",
        "\n",
        "# # Perform hyperparameter tuning\n",
        "# tuner.search_space = {\n",
        "#     'batch_size': kt.IntHyperband(32, 128, 2),\n",
        "#     'epochs': kt.IntHyperband(10, 30, 2),\n",
        "#     'learning_rate': kt.RealHyperband(0.001, 0.1, 2)\n",
        "# }\n",
        "\n",
        "# tuner.run_trial()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the model architecture\n",
        "# def build_model(input_shape, num_classes):\n",
        "#     model = Sequential([\n",
        "#         Dense(128, activation='relu', input_shape=input_shape),\n",
        "#         Dropout(0.2),\n",
        "#         Dense(64, activation='relu'),\n",
        "#         Dropout(0.2),\n",
        "#         Dense(num_classes, activation='softmax')  # Assuming multi-class classification\n",
        "#     ])\n",
        "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "\n",
        "\n",
        "# # Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     model = build_model(input_shape, num_classes)\n",
        "\n",
        "#     early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "#     model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=150, batch_size=64, callbacks=[early_stopping])\n",
        "\n",
        "#     # Save the trained model to Google Drive\n",
        "#     model_path = f'/content/drive/MyDrive/trained_6G_model1_{label_col}.h5'\n",
        "#     model.save(model_path)\n",
        "#     print(f\"Trained model saved to {model_path}\")\n",
        "\n",
        "#     # Evaluate on test set\n",
        "#     y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "#     y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "#     print(f\"Metrics for {label_col} dataset:\")\n",
        "#     print_metrics(y_true, y_pred)\n",
        "\n",
        "#     # Free up memory\n",
        "#     del X, y, X_train, y_train, X_val, y_val, X_test, y_test, model\n",
        "#     gc.collect()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Dropout, BatchNormalization\n",
        "# from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "# from keras.utils import to_categorical\n",
        "# import gc\n",
        "# from google.colab import drive\n",
        "\n",
        "\n",
        "# def print_metrics(y_true, y_pred):\n",
        "#     accuracy = accuracy_score(y_true, y_pred)\n",
        "#     precision = precision_score(y_true, y_pred, average='weighted')\n",
        "#     recall = recall_score(y_true, y_pred, average='weighted')\n",
        "#     f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "#     confusion_mat = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "#     print(f\"Accuracy: {accuracy:.4f}\")\n",
        "#     print(f\"Precision: {precision:.4f}\")\n",
        "#     print(f\"Recall: {recall:.4f}\")\n",
        "#     print(f\"F1-Score: {f1:.4f}\")\n",
        "#     print(\"Confusion Matrix:\")\n",
        "#     print(confusion_mat)\n",
        "#     print()\n",
        "\n",
        "\n",
        "# def build_model(input_shape, num_classes):\n",
        "#     model = Sequential([\n",
        "#         Dense(128, activation='relu', input_shape=input_shape),\n",
        "#         BatchNormalization(),\n",
        "#         Dropout(0.2),\n",
        "#         Dense(64, activation='relu'),\n",
        "#         BatchNormalization(),\n",
        "#         Dropout(0.2),\n",
        "#         Dense(num_classes, activation='softmax')\n",
        "#     ])\n",
        "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# def process_data(dataset, feature_cols, label_col):\n",
        "#     X = pd.get_dummies(dataset[feature_cols])\n",
        "#     y = dataset[label_col]\n",
        "\n",
        "#     label_encoder = LabelEncoder()\n",
        "#     y = label_encoder.fit_transform(y)\n",
        "\n",
        "#     num_classes = len(label_encoder.classes_)\n",
        "\n",
        "#     X = X.astype('float32')\n",
        "#     y = to_categorical(y, num_classes=num_classes).astype('float32')\n",
        "\n",
        "#     X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "#     X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "#     input_shape = (X_train.shape[1],)\n",
        "\n",
        "#     return X_train, y_train, X_val, y_val, X_test, y_test, input_shape, num_classes\n",
        "\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Train and fine-tune on each dataset\n",
        "# datasets = [nsl_kdd, unsw_nb15]\n",
        "# label_cols = ['label', 'Label']\n",
        "# unsw_col_names = [\"srcip\", \"sport\", \"dstip\", \"dsport\", \"proto\", \"state\", \"dur\", \"sbytes\", \"dbytes\", \"sttl\", \"dttl\", \"sloss\", \"dloss\", \"service\", \"Sload\", \"Dload\", \"Spkts\",\n",
        "#                   \"Dpkts\", \"swin\", \"dwin\", \"stcpb\", \"dtcpb\", \"smeansz\", \"dmeansz\", \"trans_depth\", \"res_bdy_len\", \"Sjit\", \"Djit\", \"Stime\", \"Ltime\", \"Sintpkt\", \"Dintpkt\", \"tcprtt\",\n",
        "#                   \"synack\", \"ackdat\", \"is_sm_ips_ports\", \"ct_state_ttl\", \"ct_flw_http_mthd\", \"is_ftp_login\", \"ct_ftp_cmd\", \"ct_srv_src\", \"ct_srv_dst\", \"ct_dst_ltm\", \"ct_src_ltm\",\n",
        "#                   \"ct_src_dport_ltm\", \"ct_dst_sport_ltm\", \"ct_dst_src_ltm\", \"attack_cat\"]\n",
        "# nsl_kdd_col_names = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\",\n",
        "#                      \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
        "#                      \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
        "#                      \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
        "#                      \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\"]\n",
        "# feature_cols = [nsl_kdd_col_names, unsw_col_names[:-1]]  # Exclude the 'Label' column\n",
        "\n",
        "# for dataset, label_col, feature_cols in zip(datasets, label_cols, feature_cols):\n",
        "#     print(f\"Training on dataset: {label_col}\")\n",
        "\n",
        "#     X_train, y_train, X_val, y_val, X_test, y_test, input_shape, num_classes = process_data(dataset, feature_cols, label_col)\n",
        "\n",
        "\n",
        "#     scaler = StandardScaler()\n",
        "#     X_train = scaler.fit_transform(X_train)\n",
        "#     X_val = scaler.transform(X_val)\n",
        "#     X_test = scaler.transform(X_test)\n",
        "\n",
        "#     model = build_model(input_shape, num_classes)\n",
        "\n",
        "#     early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "#     reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, min_lr=0.001)\n",
        "\n",
        "#     model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=120, batch_size=64, callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "#     model_path = f'/content/drive/MyDrive/trained_model2_{label_col}.h5'\n",
        "#     model.save(model_path)\n",
        "#     print(f\"Trained model saved to {model_path}\")\n",
        "\n",
        "#     y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "#     y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "#     print(f\"Metrics for {label_col} dataset:\")\n",
        "#     print_metrics(y_true, y_pred)\n",
        "\n",
        "#     del X_train, y_train, X_val, y_val, X_test, y_test, model\n",
        "#     gc.collect()\n",
        "\n",
        "#     import psutil\n",
        "#     process = psutil.Process()\n",
        "#     if process.memory_info().rss > 1000000000:  # 1 GB\n",
        "#         print(\"Runtime is still running, exiting...\")\n",
        "#         exit(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIcHU6AKyD8u"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCLwbgaayGOz"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Dropout, BatchNormalization\n",
        "# from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "# from keras.utils import to_categorical\n",
        "# import gc\n",
        "# from google.colab import drive\n",
        "\n",
        "# def print_metrics(y_true, y_pred):\n",
        "#     accuracy = accuracy_score(y_true, y_pred)\n",
        "#     precision = precision_score(y_true, y_pred, average='weighted')\n",
        "#     recall = recall_score(y_true, y_pred, average='weighted')\n",
        "#     f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "#     confusion_mat = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "#     print(f\"Accuracy: {accuracy:.4f}\")\n",
        "#     print(f\"Precision: {precision:.4f}\")\n",
        "#     print(f\"Recall: {recall:.4f}\")\n",
        "#     print(f\"F1-Score: {f1:.4f}\")\n",
        "#     print(\"Confusion Matrix:\")\n",
        "#     print(confusion_mat)\n",
        "#     print()\n",
        "\n",
        "# def build_model(input_shape, num_classes):\n",
        "#     model = Sequential([\n",
        "#         Dense(128, activation='relu', input_shape=input_shape),\n",
        "#         BatchNormalization(),\n",
        "#         Dropout(0.2),\n",
        "#         Dense(64, activation='relu'),\n",
        "#         BatchNormalization(),\n",
        "#         Dropout(0.2),\n",
        "#         Dense(num_classes, activation='softmax')\n",
        "#     ])\n",
        "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# def process_data(dataset, feature_cols, label_col):\n",
        "#     X = pd.get_dummies(dataset[feature_cols])\n",
        "#     y = dataset[label_col]\n",
        "\n",
        "#     y, unique_labels = pd.factorize(y)\n",
        "\n",
        "#     num_classes = len(unique_labels)\n",
        "\n",
        "#     X = X.astype('float32')\n",
        "#     y = to_categorical(y, num_classes=num_classes).astype('float32')\n",
        "\n",
        "#     X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "#     X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "#     input_shape = (X_train.shape[1],)\n",
        "\n",
        "#     return X_train, y_train, X_val, y_val, X_test, y_test, input_shape, num_classes\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Train and fine-tune on the dataset\n",
        "# data_dir = '/content/drive/MyDrive/6data/CIC_IDS17/dataB/processed/'\n",
        "# processed_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.startswith('processed_chunk_')]\n",
        "\n",
        "# feature_cols = [\"Flow ID\", \"Source IP\", \"Source Port\", \"Destination IP\", \"Destination Port\", \"Protocol\", \"Timestamp\", \"Flow Duration\", \"Total Fwd Packets\", \"Total Backward Packets\", \"Total Length of Fwd Packets\", \"Total Length of Bwd Packets\", \"Fwd Packet Length Max\", \"Fwd Packet Length Min\", \"Fwd Packet Length Mean\", \"Fwd Packet Length Std\", \"Bwd Packet Length Max\", \"Bwd Packet Length Min\", \"Bwd Packet Length Mean\", \"Bwd Packet Length Std\", \"Flow Bytes/s\", \"Flow Packets/s\", \"Flow IAT Mean\", \"Flow IAT Std\", \"Flow IAT Max\", \"Flow IAT Min\", \"Fwd IAT Total\", \"Fwd IAT Mean\", \"Fwd IAT Std\", \"Fwd IAT Max\", \"Fwd IAT Min\", \"Bwd IAT Total\", \"Bwd IAT Mean\", \"Bwd IAT Std\", \"Bwd IAT Max\", \"Bwd IAT Min\", \"Fwd PSH Flags\", \"Bwd PSH Flags\", \"Fwd URG Flags\", \"Bwd URG Flags\", \"Fwd Header Length\", \"Bwd Header Length\", \"Fwd Packets/s\", \"Bwd Packets/s\", \"Min Packet Length\", \"Max Packet Length\", \"Packet Length Mean\", \"Packet Length Std\", \"Packet Length Variance\", \"FIN Flag Count\", \"SYN Flag Count\", \"RST Flag Count\", \"PSH Flag Count\", \"ACK Flag Count\", \"URG Flag Count\", \"CWE Flag Count\", \"ECE Flag Count\", \"Down/Up Ratio\", \"Average Packet Size\", \"Avg Fwd Segment Size\", \"Avg Bwd Segment Size\", \"Fwd Header Length\", \"Fwd Avg Bytes/Bulk\", \"Fwd Avg Packets/Bulk\", \"Fwd Avg Bulk Rate\", \"Bwd Avg Bytes/Bulk\", \"Bwd Avg Packets/Bulk\", \"Bwd Avg Bulk Rate\", \"Subflow Fwd Packets\", \"Subflow Fwd Bytes\", \"Subflow Bwd Packets\", \"Subflow Bwd Bytes\", \"Init_Win_bytes_forward\", \"Init_Win_bytes_backward\", \"act_data_pkt_fwd\", \"min_seg_size_forward\", \"Active Mean\", \"Active Std\", \"Active Max\", \"Active Min\", \"Idle Mean\", \"Idle Std\", \"Idle Max\", \"Idle Min\"]\n",
        "# label_col = 'Label'\n",
        "\n",
        "# dtypes = {\n",
        "#     \"Flow ID\": str,\n",
        "#     \"Source IP\": str,\n",
        "#     \"Destination IP\": str,\n",
        "#     \"Protocol\": 'category',\n",
        "#     \"Flow Duration\": float,\n",
        "#     \"Total Fwd Packets\": int,\n",
        "#     \"Total Backward Packets\": int,\n",
        "#     \"Total Length of Fwd Packets\": int,\n",
        "#     \"Total Length of Bwd Packets\": int,\n",
        "#     \"Fwd Packet Length Max\": float,\n",
        "#     \"Fwd Packet Length Min\": float,\n",
        "#     \"Fwd Packet Length Mean\": float,\n",
        "#     \"Fwd Packet Length Std\": float,\n",
        "#     \"Bwd Packet Length Max\": float,\n",
        "#     \"Bwd Packet Length Min\": float,\n",
        "#     \"Bwd Packet Length Mean\": float,\n",
        "#     \"Bwd Packet Length Std\": float,\n",
        "#     \"Flow Bytes/s\": float,\n",
        "#     \"Flow Packets/s\": float,\n",
        "#     \"Flow IAT Mean\": float,\n",
        "#     \"Flow IAT Std\": float,\n",
        "#     \"Flow IAT Max\": float,\n",
        "#     \"Flow IAT Min\": float,\n",
        "#     \"Fwd IAT Total\": float,\n",
        "#     \"Fwd IAT Mean\": float,\n",
        "#     \"Fwd IAT Std\": float,\n",
        "#     \"Fwd IAT Max\": float,\n",
        "#     \"Fwd IAT Min\": float,\n",
        "#     \"Bwd IAT Total\": float,\n",
        "#     \"Bwd IAT Mean\": float,\n",
        "#     \"Bwd IAT Std\": float,\n",
        "#     \"Bwd IAT Max\": float,\n",
        "#     \"Bwd IAT Min\": float,\n",
        "#     \"Fwd PSH Flags\": int,\n",
        "#     \"Bwd PSH Flags\": int,\n",
        "#     \"Fwd URG Flags\": int,\n",
        "#     \"Bwd URG Flags\": int,\n",
        "#     \"Fwd Header Length\": int,\n",
        "#     \"Bwd Header Length\": int,\n",
        "#     \"Fwd Packets/s\": float,\n",
        "#     \"Bwd Packets/s\": float,\n",
        "#     \"Min Packet Length\": float,\n",
        "#     \"Max Packet Length\": float,\n",
        "#     \"Packet Length Mean\": float,\n",
        "#     \"Packet Length Std\": float,\n",
        "#     \"Packet Length Variance\": float,\n",
        "#     \"FIN Flag Count\": int,\n",
        "#     \"SYN Flag Count\": int,\n",
        "#     \"RST Flag Count\": int,\n",
        "#     \"PSH Flag Count\": int,\n",
        "#     \"ACK Flag Count\": int,\n",
        "#     \"URG Flag Count\": int,\n",
        "#     \"CWE Flag Count\": int,\n",
        "#     \"ECE Flag Count\": int,\n",
        "#     \"Down/Up Ratio\": float,\n",
        "#     \"Average Packet Size\": float,\n",
        "#     \"Avg Fwd Segment Size\": float,\n",
        "#     \"Avg Bwd Segment Size\": float,\n",
        "#     \"Fwd Header Length\": int,\n",
        "#     \"Fwd Avg Bytes/Bulk\": float,\n",
        "#     \"Fwd Avg Packets/Bulk\": float,\n",
        "#     \"Fwd Avg Bulk Rate\": float,\n",
        "#     \"Bwd Avg Bytes/Bulk\": float,\n",
        "#     \"Bwd Avg Packets/Bulk\": float,\n",
        "#     \"Bwd Avg Bulk Rate\": float,\n",
        "#     \"Subflow Fwd Packets\": int,\n",
        "#     \"Subflow Fwd Bytes\": int,\n",
        "#     \"Subflow Bwd Packets\": int,\n",
        "#     \"Subflow Bwd Bytes\": int,\n",
        "#     \"Init_Win_bytes_forward\": int,\n",
        "#     \"Init_Win_bytes_backward\": int,\n",
        "#     \"act_data_pkt_fwd\": int,\n",
        "#     \"min_seg_size_forward\": int,\n",
        "#     \"Active Mean\": float,\n",
        "#     \"Active Std\": float,\n",
        "#     \"Active Max\": float,\n",
        "#     \"Active Min\": float,\n",
        "#     \"Idle Mean\": float,\n",
        "#     \"Idle Std\": float,\n",
        "#     \"Idle Max\": float,\n",
        "#     \"Idle Min\": float\n",
        "# }\n",
        "\n",
        "# print(\"Training on the dataset...\")\n",
        "\n",
        "# X_train_batches = []\n",
        "# y_train_batches = []\n",
        "# X_val_batches = []\n",
        "# y_val_batches = []\n",
        "# X_test_batches = []\n",
        "# y_test_batches = []\n",
        "\n",
        "# for file_path in processed_files:\n",
        "#     data = pd.read_csv(file_path, dtype=dtypes)\n",
        "#     data['Flow Duration'] = data['Flow Duration'].replace(' ', np.nan).astype(float)  # Handle string values in \"Flow Duration\"\n",
        "#     X = data[feature_cols]\n",
        "#     y = data[label_col]\n",
        "\n",
        "#     X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "#     X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "#     X_train_batches.append(X_train)\n",
        "#     y_train_batches.append(y_train)\n",
        "#     X_val_batches.append(X_val)\n",
        "#     y_val_batches.append(y_val)\n",
        "#     X_test_batches.append(X_test)\n",
        "#     y_test_batches.append(y_test)\n",
        "\n",
        "# X_train = pd.concat(X_train_batches, ignore_index=True)\n",
        "# y_train = pd.concat(y_train_batches, ignore_index=True)\n",
        "# X_val = pd.concat(X_val_batches, ignore_index=True)\n",
        "# y_val = pd.concat(y_val_batches, ignore_index=True)\n",
        "# X_test = pd.concat(X_test_batches, ignore_index=True)\n",
        "# y_test = pd.concat(y_test_batches, ignore_index=True)\n",
        "\n",
        "# y_train, unique_labels = pd.factorize(y_train)\n",
        "# y_val = pd.factorize(y_val, sort=True)[0]\n",
        "# y_test = pd.factorize(y_test, sort=True)[0]\n",
        "\n",
        "# num_classes = len(unique_labels)\n",
        "\n",
        "# X_train = X_train.astype('float32')\n",
        "# X_val = X_val.astype('float32')\n",
        "# X_test = X_test.astype('float32')\n",
        "# y_train = to_categorical(y_train, num_classes=num_classes).astype('float32')\n",
        "# y_val = to_categorical(y_val, num_classes=num_classes).astype('float32')\n",
        "# y_test = to_categorical(y_test, num_classes=num_classes).astype('float32')\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_val = scaler.transform(X_val)\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "# input_shape = (X_train.shape[1],)\n",
        "\n",
        "# model = build_model(input_shape, num_classes)\n",
        "\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, min_lr=0.001)\n",
        "\n",
        "# model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=120, batch_size=64, callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "# model_path = '/content/drive/MyDrive/trained_6G_model.h5'\n",
        "# model.save(model_path)\n",
        "# print(f\"Trained model saved to {model_path}\")\n",
        "\n",
        "# y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "# y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# print(\"Metrics for the dataset:\")\n",
        "# print_metrics(y_true, y_pred)\n",
        "\n",
        "# del X_train, y_train, X_val, y_val, X_test, y_test, model\n",
        "# gc.collect()\n",
        "\n",
        "# import psutil\n",
        "# process = psutil.Process()\n",
        "# if process.memory_info().rss > 1000000000:  # 1 GB\n",
        "#     print(\"Runtime is still running, exiting...\")\n",
        "#     exit(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9diPNNWcyFLJ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ND-r5bRRxcy"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import gc\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Dropout, BatchNormalization\n",
        "# from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "# from keras.utils import to_categorical\n",
        "# from google.colab import drive\n",
        "# import pickle\n",
        "\n",
        "# def print_metrics(y_true, y_pred):\n",
        "#     from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "#     accuracy = accuracy_score(y_true, y_pred)\n",
        "#     precision = precision_score(y_true, y_pred, average='weighted')\n",
        "#     recall = recall_score(y_true, y_pred, average='weighted')\n",
        "#     f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "#     confusion_mat = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "#     print(f\"Accuracy: {accuracy:.4f}\")\n",
        "#     print(f\"Precision: {precision:.4f}\")\n",
        "#     print(f\"Recall: {recall:.4f}\")\n",
        "#     print(f\"F1-Score: {f1:.4f}\")\n",
        "#     print(\"Confusion Matrix:\")\n",
        "#     print(confusion_mat)\n",
        "#     print()\n",
        "\n",
        "# def build_model(input_shape, num_classes):\n",
        "#     model = Sequential([\n",
        "#         Dense(128, activation='relu', input_shape=input_shape),\n",
        "#         BatchNormalization(),\n",
        "#         Dropout(0.2),\n",
        "#         Dense(64, activation='relu'),\n",
        "#         BatchNormalization(),\n",
        "#         Dropout(0.2),\n",
        "#         Dense(num_classes, activation='softmax')\n",
        "#     ])\n",
        "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Train and fine-tune on the dataset\n",
        "# data_dir = '/content/drive/MyDrive/6data/CIC_IDS17/dataB/processed/'\n",
        "# processed_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.startswith('processed_chunk_')]\n",
        "\n",
        "# # Placeholder for feature_cols\n",
        "# feature_cols = [\"Flow ID\", \"Source IP\", \"Destination IP\", \"Timestamp\", \"Source Port\", \"Destination Port\", \"Protocol\", \"Flow Duration\", \"Total Fwd Packets\", \"Total Backward Packets\", \"Total Length of Fwd Packets\", \"Total Length of Bwd Packets\", \"Fwd Packet Length Max\", \"Fwd Packet Length Min\", \"Fwd Packet Length Mean\", \"Fwd Packet Length Std\", \"Bwd Packet Length Max\", \"Bwd Packet Length Min\", \"Bwd Packet Length Mean\", \"Bwd Packet Length Std\", \"Flow Bytes/s\", \"Flow Packets/s\", \"Flow IAT Mean\", \"Flow IAT Std\", \"Flow IAT Max\", \"Flow IAT Min\", \"Fwd IAT Total\", \"Fwd IAT Mean\", \"Fwd IAT Std\", \"Fwd IAT Max\", \"Fwd IAT Min\", \"Bwd IAT Total\", \"Bwd IAT Mean\", \"Bwd IAT Std\", \"Bwd IAT Max\", \"Bwd IAT Min\", \"Fwd PSH Flags\", \"Bwd PSH Flags\", \"Fwd URG Flags\", \"Bwd URG Flags\", \"Fwd Header Length\", \"Bwd Header Length\", \"Fwd Packets/s\", \"Bwd Packets/s\", \"Min Packet Length\", \"Max Packet Length\", \"Packet Length Mean\", \"Packet Length Std\", \"Packet Length Variance\", \"FIN Flag Count\", \"SYN Flag Count\", \"RST Flag Count\", \"PSH Flag Count\", \"ACK Flag Count\", \"URG Flag Count\", \"CWE Flag Count\", \"ECE Flag Count\", \"Down/Up Ratio\", \"Average Packet Size\", \"Avg Fwd Segment Size\", \"Avg Bwd Segment Size\", \"Fwd Avg Bytes/Bulk\", \"Fwd Avg Packets/Bulk\", \"Fwd Avg Bulk Rate\", \"Bwd Avg Bytes/Bulk\", \"Bwd Avg Packets/Bulk\", \"Bwd Avg Bulk Rate\", \"Subflow Fwd Packets\", \"Subflow Fwd Bytes\", \"Subflow Bwd Packets\", \"Subflow Bwd Bytes\", \"Init_Win_bytes_forward\", \"Init_Win_bytes_backward\", \"act_data_pkt_fwd\", \"min_seg_size_forward\", \"Active Mean\", \"Active Std\", \"Active Max\", \"Active Min\", \"Idle Mean\", \"Idle Std\", \"Idle Max\", \"Idle Min\"]\n",
        "# label_col = 'Label'\n",
        "\n",
        "\n",
        "# dtypes = {\n",
        "#     \"Flow ID\": str,\n",
        "#     \"Source IP\": str,\n",
        "#     \"Destination IP\": str,\n",
        "#     \"Protocol\": 'category',\n",
        "#     \"Flow Duration\": object,\n",
        "#     \"Total Fwd Packets\": object,\n",
        "#     \"Total Backward Packets\": object,\n",
        "#     \"Total Length of Fwd Packets\": object,\n",
        "#     \"Total Length of Bwd Packets\": object,\n",
        "#     \"Fwd Packet Length Max\": object,\n",
        "#     \"Fwd Packet Length Min\": object,\n",
        "#     \"Fwd Packet Length Mean\": object,\n",
        "#     \"Fwd Packet Length Std\": object,\n",
        "#     \"Bwd Packet Length Max\": object,\n",
        "#     \"Bwd Packet Length Min\": object,\n",
        "#     \"Bwd Packet Length Mean\": object,\n",
        "#     \"Bwd Packet Length Std\": object,\n",
        "#     \"Flow Bytes/s\": object,\n",
        "#     \"Flow Packets/s\": object,\n",
        "#     \"Flow IAT Mean\": object,\n",
        "#     \"Flow IAT Std\": object,\n",
        "#     \"Flow IAT Max\": object,\n",
        "#     \"Flow IAT Min\": object,\n",
        "#     \"Fwd IAT Total\": object,\n",
        "#     \"Fwd IAT Mean\": object,\n",
        "#     \"Fwd IAT Std\": object,\n",
        "#     \"Fwd IAT Max\": object,\n",
        "#     \"Fwd IAT Min\": object,\n",
        "#     \"Bwd IAT Total\": object,\n",
        "#     \"Bwd IAT Mean\": object,\n",
        "#     \"Bwd IAT Std\": object,\n",
        "#     \"Bwd IAT Max\": object,\n",
        "#     \"Bwd IAT Min\": object,\n",
        "#     \"Fwd PSH Flags\": object,\n",
        "#     \"Bwd PSH Flags\": object,\n",
        "#     \"Fwd URG Flags\": object,\n",
        "#     \"Bwd URG Flags\": object,\n",
        "#     \"Fwd Header Length\": object,\n",
        "#     \"Bwd Header Length\": object,\n",
        "#     \"Fwd Packets/s\": object,\n",
        "#     \"Bwd Packets/s\": object,\n",
        "#     \"Min Packet Length\": object,\n",
        "#     \"Max Packet Length\": object,\n",
        "#     \"Packet Length Mean\": object,\n",
        "#     \"Packet Length Std\": object,\n",
        "#     \"Packet Length Variance\": object,\n",
        "#     \"FIN Flag Count\": object,\n",
        "#     \"SYN Flag Count\": object,\n",
        "#     \"RST Flag Count\": object,\n",
        "#     \"PSH Flag Count\": object,\n",
        "#     \"ACK Flag Count\": object,\n",
        "#     \"URG Flag Count\": object,\n",
        "#     \"CWE Flag Count\": object,\n",
        "#     \"ECE Flag Count\": object,\n",
        "#     \"Down/Up Ratio\": object,\n",
        "#     \"Average Packet Size\": object,\n",
        "#     \"Avg Fwd Segment Size\": object,\n",
        "#     \"Avg Bwd Segment Size\": object,\n",
        "#     \"Fwd Avg Bytes/Bulk\": object,\n",
        "#     \"Fwd Avg Packets/Bulk\": object,\n",
        "#     \"Fwd Avg Bulk Rate\": object,\n",
        "#     \"Bwd Avg Bytes/Bulk\": object,\n",
        "#     \"Bwd Avg Packets/Bulk\": object,\n",
        "#     \"Bwd Avg Bulk Rate\": object,\n",
        "#     \"Subflow Fwd Packets\": object,\n",
        "#     \"Subflow Fwd Bytes\": object,\n",
        "#     \"Subflow Bwd Packets\": object,\n",
        "#     \"Subflow Bwd Bytes\": object,\n",
        "#     \"Init_Win_bytes_forward\": object,\n",
        "#     \"Init_Win_bytes_backward\": object,\n",
        "#     \"act_data_pkt_fwd\": object,\n",
        "#     \"min_seg_size_forward\": object,\n",
        "#     \"Active Mean\": object,\n",
        "#     \"Active Std\": object,\n",
        "#     \"Active Max\": object,\n",
        "#     \"Active Min\": object,\n",
        "#     \"Idle Mean\": object,\n",
        "#     \"Idle Std\": object,\n",
        "#     \"Idle Max\": object,\n",
        "#     \"Idle Min\": object\n",
        "# }\n",
        "\n",
        "# print(\"Training on the dataset...\")\n",
        "\n",
        "# X_train_batches = []\n",
        "# y_train_batches = []\n",
        "# X_val_batches = []\n",
        "# y_val_batches = []\n",
        "# X_test_batches = []\n",
        "# y_test_batches = []\n",
        "\n",
        "# for file_path in processed_files:\n",
        "#     data = pd.read_csv(file_path, dtype=dtypes)\n",
        "#     data['Flow Duration'] = data['Flow Duration'].replace(' ', np.nan)\n",
        "#     data['Total Fwd Packets'] = data['Total Fwd Packets'].replace(' ', np.nan)\n",
        "#     data['Total Backward Packets'] = data['Total Backward Packets'].replace(' ', np.nan)\n",
        "#     data['Total Length of Fwd Packets'] = data['Total Length of Fwd Packets'].replace(' ', np.nan)\n",
        "#     data['Total Length of Bwd Packets'] = data['Total Length of Bwd Packets'].replace(' ', np.nan)\n",
        "#     data['Fwd Packet Length Max'] = data['Fwd Packet Length Max'].replace(' ', np.nan)\n",
        "#     data['Fwd Packet Length Min'] = data['Fwd Packet Length Min'].replace(' ', np.nan)\n",
        "#     data['Fwd Packet Length Mean'] = data['Fwd Packet Length Mean'].replace(' ', np.nan)\n",
        "#     data['Fwd Packet Length Std'] = data['Fwd Packet Length Std'].replace(' ', np.nan)\n",
        "#     data['Bwd Packet Length Max'] = data['Bwd Packet Length Max'].replace(' ', np.nan)\n",
        "#     data['Bwd Packet Length Min'] = data['Bwd Packet Length Min'].replace(' ', np.nan)\n",
        "#     data['Bwd Packet Length Mean'] = data['Bwd Packet Length Mean'].replace(' ', np.nan)\n",
        "#     data['Bwd Packet Length Std'] = data['Bwd Packet Length Std'].replace(' ', np.nan)\n",
        "#     data['Flow Bytes/s'] = data['Flow Bytes/s'].replace(' ', np.nan)\n",
        "#     data['Flow Packets/s'] = data['Flow Packets/s'].replace(' ', np.nan)\n",
        "#     data['Flow IAT Mean'] = data['Flow IAT Mean'].replace(' ', np.nan)\n",
        "#     data['Flow IAT Std'] = data['Flow IAT Std'].replace(' ', np.nan)\n",
        "#     data['Flow IAT Max'] = data['Flow IAT Max'].replace(' ', np.nan)\n",
        "#     data['Flow IAT Min'] = data['Flow IAT Min'].replace(' ', np.nan)\n",
        "#     data['Fwd IAT Total'] = data['Fwd IAT Total'].replace(' ', np.nan)\n",
        "#     data['Fwd IAT Mean'] = data['Fwd IAT Mean'].replace(' ', np.nan)\n",
        "#     data['Fwd IAT Std'] = data['Fwd IAT Std'].replace(' ', np.nan)\n",
        "#     data['Fwd IAT Max'] = data['Fwd IAT Max'].replace(' ', np.nan)\n",
        "#     data['Fwd IAT Min'] = data['Fwd IAT Min'].replace(' ', np.nan)\n",
        "#     data['Bwd IAT Total'] = data['Bwd IAT Total'].replace(' ', np.nan)\n",
        "#     data['Bwd IAT Mean'] = data['Bwd IAT Mean'].replace(' ', np.nan)\n",
        "#     data['Bwd IAT Std'] = data['Bwd IAT Std'].replace(' ', np.nan)\n",
        "#     data['Bwd IAT Max'] = data['Bwd IAT Max'].replace(' ', np.nan)\n",
        "#     data['Bwd IAT Min'] = data['Bwd IAT Min'].replace(' ', np.nan)\n",
        "#     data['Fwd PSH Flags'] = data['Fwd PSH Flags'].replace(' ', np.nan)\n",
        "#     data['Bwd PSH Flags'] = data['Bwd PSH Flags'].replace(' ', np.nan)\n",
        "#     data['Fwd URG Flags'] = data['Fwd URG Flags'].replace(' ', np.nan)\n",
        "#     data['Bwd URG Flags'] = data['Bwd URG Flags'].replace(' ', np.nan)\n",
        "#     data['Fwd Header Length'] = data['Fwd Header Length'].replace(' ', np.nan)\n",
        "#     data['Bwd Header Length'] = data['Bwd Header Length'].replace(' ', np.nan)\n",
        "#     data['Fwd Packets/s'] = data['Fwd Packets/s'].replace(' ', np.nan)\n",
        "#     data['Bwd Packets/s'] = data['Bwd Packets/s'].replace(' ', np.nan)\n",
        "#     data['Min Packet Length'] = data['Min Packet Length'].replace(' ', np.nan)\n",
        "#     data['Max Packet Length'] = data['Max Packet Length'].replace(' ', np.nan)\n",
        "#     data['Packet Length Mean'] = data['Packet Length Mean'].replace(' ', np.nan)\n",
        "#     data['Packet Length Std'] = data['Packet Length Std'].replace(' ', np.nan)\n",
        "#     data['Packet Length Variance'] = data['Packet Length Variance'].replace(' ', np.nan)\n",
        "#     data['FIN Flag Count'] = data['FIN Flag Count'].replace(' ', np.nan)\n",
        "#     data['SYN Flag Count'] = data['SYN Flag Count'].replace(' ', np.nan)\n",
        "#     data['RST Flag Count'] = data['RST Flag Count'].replace(' ', np.nan)\n",
        "#     data['PSH Flag Count'] = data['PSH Flag Count'].replace(' ', np.nan)\n",
        "#     data['ACK Flag Count'] = data['ACK Flag Count'].replace(' ', np.nan)\n",
        "#     data['URG Flag Count'] = data['URG Flag Count'].replace(' ', np.nan)\n",
        "#     data['CWE Flag Count'] = data['CWE Flag Count'].replace(' ', np.nan)\n",
        "#     data['ECE Flag Count'] = data['ECE Flag Count'].replace(' ', np.nan)\n",
        "#     data['Down/Up Ratio'] = data['Down/Up Ratio'].replace(' ', np.nan)\n",
        "#     data['Average Packet Size'] = data['Average Packet Size'].replace(' ', np.nan)\n",
        "#     data['Avg Fwd Segment Size'] = data['Avg Fwd Segment Size'].replace(' ', np.nan)\n",
        "#     data['Avg Bwd Segment Size'] = data['Avg Bwd Segment Size'].replace(' ', np.nan)\n",
        "#     data['Fwd Avg Bytes/Bulk'] = data['Fwd Avg Bytes/Bulk'].replace(' ', np.nan)\n",
        "#     data['Fwd Avg Packets/Bulk'] = data['Fwd Avg Packets/Bulk'].replace(' ', np.nan)\n",
        "#     data['Fwd Avg Bulk Rate'] = data['Fwd Avg Bulk Rate'].replace(' ', np.nan)\n",
        "#     data['Bwd Avg Bytes/Bulk'] = data['Bwd Avg Bytes/Bulk'].replace(' ', np.nan)\n",
        "#     data['Bwd Avg Packets/Bulk'] = data['Bwd Avg Packets/Bulk'].replace(' ', np.nan)\n",
        "#     data['Bwd Avg Bulk Rate'] = data['Bwd Avg Bulk Rate'].replace(' ', np.nan)\n",
        "#     data['Subflow Fwd Packets'] = data['Subflow Fwd Packets'].replace(' ', np.nan)\n",
        "#     data['Subflow Fwd Bytes'] = data['Subflow Fwd Bytes'].replace(' ', np.nan)\n",
        "#     data['Subflow Bwd Packets'] = data['Subflow Bwd Packets'].replace(' ', np.nan)\n",
        "#     data['Subflow Bwd Bytes'] = data['Subflow Bwd Bytes'].replace(' ', np.nan)\n",
        "#     data['Init_Win_bytes_forward'] = data['Init_Win_bytes_forward'].replace(' ', np.nan)\n",
        "#     data['Init_Win_bytes_backward'] = data['Init_Win_bytes_backward'].replace(' ', np.nan)\n",
        "#     data['act_data_pkt_fwd'] = data['act_data_pkt_fwd'].replace(' ', np.nan)\n",
        "#     data['min_seg_size_forward'] = data['min_seg_size_forward'].replace(' ', np.nan)\n",
        "#     data['Active Mean'] = data['Active Mean'].replace(' ', np.nan)\n",
        "#     data['Active Std'] = data['Active Std'].replace(' ', np.nan)\n",
        "#     data['Active Max'] = data['Active Max'].replace(' ', np.nan)\n",
        "#     data['Active Min'] = data['Active Min'].replace(' ', np.nan)\n",
        "#     data['Idle Mean'] = data['Idle Mean'].replace(' ', np.nan)\n",
        "#     data['Idle Std'] = data['Idle Std'].replace(' ', np.nan)\n",
        "#     data['Idle Max'] = data['Idle Max'].replace(' ', np.nan)\n",
        "#     data['Idle Min'] = data['Idle Min'].replace(' ', np.nan)\n",
        "\n",
        "#     X = data[feature_cols]\n",
        "#     y = data[label_col]\n",
        "\n",
        "#     #   # Preprocess the target variable\n",
        "#     # y = y.str.strip()  # Remove leading/trailing whitespaces\n",
        "#     # y = y.str.replace(r'[^a-zA-Z0-9]', '', regex=True)  # Remove non-alphanumeric characters\n",
        "\n",
        "\n",
        "#     y = y.replace('Label', np.nan)  # Replace the string 'Label' with NaN\n",
        "#     y = y.dropna()  # Remove any remaining NaN values\n",
        "\n",
        "\n",
        "#     X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "#     X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "#     X_train_batches.append(X_train)\n",
        "#     y_train_batches.append(y_train)\n",
        "#     X_val_batches.append(X_val)\n",
        "#     y_val_batches.append(y_val)\n",
        "#     X_test_batches.append(X_test)\n",
        "#     y_test_batches.append(y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# X_train = pd.concat(X_train_batches, ignore_index=True)\n",
        "# y_train = pd.concat(y_train_batches, ignore_index=True)\n",
        "# X_val = pd.concat(X_val_batches, ignore_index=True)\n",
        "# y_val = pd.concat(y_val_batches, ignore_index=True)\n",
        "# X_test = pd.concat(X_test_batches, ignore_index=True)\n",
        "# y_test = pd.concat(y_test_batches, ignore_index=True)\n",
        "\n",
        "# # Encode the target variable\n",
        "# le = LabelEncoder()\n",
        "# y_train = le.fit_transform(y_train)\n",
        "# y_val = le.transform(y_val)\n",
        "# y_test = le.transform(y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Get the unique classes from the combined training and validation sets\n",
        "# unique_classes = np.unique(np.concatenate((y_train, y_val)))\n",
        "# num_classes = len(unique_classes)\n",
        "\n",
        "# # Convert the target variables to one-hot encoded format\n",
        "# y_train = to_categorical(y_train, num_classes=num_classes).astype('float32')\n",
        "# y_val = to_categorical(y_val, num_classes=num_classes).astype('float32')\n",
        "\n",
        "# # Handle unseen classes in the test set\n",
        "# y_test_encoded = np.zeros((y_test.shape[0], num_classes), dtype='float32')\n",
        "# for i, label in enumerate(y_test):\n",
        "#     if label in unique_classes:\n",
        "#         y_test_encoded[i, label] = 1\n",
        "#     else:\n",
        "#         # Handle unseen classes by setting them to a separate class\n",
        "#         y_test_encoded[i, -1] = 1\n",
        "\n",
        "# # Ensure the column order is correct\n",
        "# X_train = X_train[feature_cols]\n",
        "# X_val = X_val[feature_cols]\n",
        "# X_test = X_test[feature_cols]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# flow_id_column = X_train['Flow ID']\n",
        "# source_ip_column = X_train['Source IP']\n",
        "# dest_ip_column = X_train['Destination IP']\n",
        "# timestamp_column = X_train['Timestamp']\n",
        "# source_port_column = X_train['Source Port']\n",
        "# dest_port_column = X_train['Destination Port']\n",
        "# protocol_column = X_train['Protocol']\n",
        "# flow_duration_column = X_train['Flow Duration']\n",
        "# total_fwd_packets_column = X_train['Total Fwd Packets']\n",
        "# total_bwd_packets_column = X_train['Total Backward Packets']\n",
        "# total_fwd_length_column = X_train['Total Length of Fwd Packets']\n",
        "# total_bwd_length_column = X_train['Total Length of Bwd Packets']\n",
        "# fwd_pkt_len_max_column = X_train['Fwd Packet Length Max']\n",
        "# fwd_pkt_len_min_column = X_train['Fwd Packet Length Min']\n",
        "# fwd_pkt_len_mean_column = X_train['Fwd Packet Length Mean']\n",
        "# fwd_pkt_len_std_column = X_train['Fwd Packet Length Std']\n",
        "# bwd_pkt_len_max_column = X_train['Bwd Packet Length Max']\n",
        "# bwd_pkt_len_min_column = X_train['Bwd Packet Length Min']\n",
        "# bwd_pkt_len_mean_column = X_train['Bwd Packet Length Mean']\n",
        "# bwd_pkt_len_std_column = X_train['Bwd Packet Length Std']\n",
        "# flow_bytes_column = X_train['Flow Bytes/s']\n",
        "# flow_packets_column = X_train['Flow Packets/s']\n",
        "# flow_iat_mean_column = X_train['Flow IAT Mean']\n",
        "# flow_iat_std_column = X_train['Flow IAT Std']\n",
        "# flow_iat_max_column = X_train['Flow IAT Max']\n",
        "# flow_iat_min_column = X_train['Flow IAT Min']\n",
        "# fwd_iat_total_column = X_train['Fwd IAT Total']\n",
        "# fwd_iat_mean_column = X_train['Fwd IAT Mean']\n",
        "# fwd_iat_std_column = X_train['Fwd IAT Std']\n",
        "# fwd_iat_max_column = X_train['Fwd IAT Max']\n",
        "# fwd_iat_min_column = X_train['Fwd IAT Min']\n",
        "# bwd_iat_total_column = X_train['Bwd IAT Total']\n",
        "# bwd_iat_mean_column = X_train['Bwd IAT Mean']\n",
        "# bwd_iat_std_column = X_train['Bwd IAT Std']\n",
        "# bwd_iat_max_column = X_train['Bwd IAT Max']\n",
        "# bwd_iat_min_column = X_train['Bwd IAT Min']\n",
        "# fwd_psh_flags_column = X_train['Fwd PSH Flags']\n",
        "# bwd_psh_flags_column = X_train['Bwd PSH Flags']\n",
        "# fwd_urg_flags_column = X_train['Fwd URG Flags']\n",
        "# bwd_urg_flags_column = X_train['Bwd URG Flags']\n",
        "# fwd_header_length_column = X_train['Fwd Header Length']\n",
        "# bwd_header_length_column = X_train['Bwd Header Length']\n",
        "# fwd_packets_column = X_train['Fwd Packets/s']\n",
        "# bwd_packets_column = X_train['Bwd Packets/s']\n",
        "# min_pkt_length_column = X_train['Min Packet Length']\n",
        "# max_pkt_length_column = X_train['Max Packet Length']\n",
        "# pkt_length_mean_column = X_train['Packet Length Mean']\n",
        "# pkt_length_std_column = X_train['Packet Length Std']\n",
        "# pkt_length_variance_column = X_train['Packet Length Variance']\n",
        "# fin_flag_count_column = X_train['FIN Flag Count']\n",
        "# syn_flag_count_column = X_train['SYN Flag Count']\n",
        "# rst_flag_count_column = X_train['RST Flag Count']\n",
        "# psh_flag_count_column = X_train['PSH Flag Count']\n",
        "# ack_flag_count_column = X_train['ACK Flag Count']\n",
        "# urg_flag_count_column = X_train['URG Flag Count']\n",
        "# cwe_flag_count_column = X_train['CWE Flag Count']\n",
        "# ece_flag_count_column = X_train['ECE Flag Count']\n",
        "# down_up_ratio_column = X_train['Down/Up Ratio']\n",
        "# avg_pkt_size_column = X_train['Average Packet Size']\n",
        "# avg_fwd_segment_column = X_train['Avg Fwd Segment Size']\n",
        "# avg_bwd_segment_column = X_train['Avg Bwd Segment Size']\n",
        "# fwd_avg_bytes_bulk_column = X_train['Fwd Avg Bytes/Bulk']\n",
        "# fwd_avg_packets_bulk_column = X_train['Fwd Avg Packets/Bulk']\n",
        "# fwd_avg_bulk_rate_column = X_train['Fwd Avg Bulk Rate']\n",
        "# bwd_avg_bytes_bulk_column = X_train['Bwd Avg Bytes/Bulk']\n",
        "# bwd_avg_packets_bulk_column = X_train['Bwd Avg Packets/Bulk']\n",
        "# bwd_avg_bulk_rate_column = X_train['Bwd Avg Bulk Rate']\n",
        "# subflow_fwd_packets_column = X_train['Subflow Fwd Packets']\n",
        "# subflow_fwd_bytes_column = X_train['Subflow Fwd Bytes']\n",
        "# subflow_bwd_packets_column = X_train['Subflow Bwd Packets']\n",
        "# subflow_bwd_bytes_column = X_train['Subflow Bwd Bytes']\n",
        "# init_win_bytes_fwd_column = X_train['Init_Win_bytes_forward']\n",
        "# init_win_bytes_bwd_column = X_train['Init_Win_bytes_backward']\n",
        "# act_data_pkt_fwd_column = X_train['act_data_pkt_fwd']\n",
        "# min_seg_size_fwd_column = X_train['min_seg_size_forward']\n",
        "# active_mean_column = X_train['Active Mean']\n",
        "# active_std_column = X_train['Active Std']\n",
        "# active_max_column = X_train['Active Max']\n",
        "# active_min_column = X_train['Active Min']\n",
        "# idle_mean_column = X_train['Idle Mean']\n",
        "# idle_std_column = X_train['Idle Std']\n",
        "# idle_max_column = X_train['Idle Max']\n",
        "# idle_min_column = X_train['Idle Min']\n",
        "\n",
        "\n",
        "\n",
        "# # Convert the remaining columns to float32\n",
        "# X_train = X_train.drop(['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Source Port', 'Destination Port', 'Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward','Active Mean','Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min' ], axis=1).astype('float32')\n",
        "# X_val = X_val.drop(['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Source Port', 'Destination Port', 'Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward','Active Mean','Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min' ], axis=1).astype('float32')\n",
        "# X_test = X_test.drop(['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Source Port', 'Destination Port', 'Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward','Active Mean','Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min' ], axis=1).astype('float32')\n",
        "\n",
        "# # Concatenate the 'Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Source Port', 'Destination Port', 'Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min' columns back to the DataFrames\n",
        "# X_train = pd.concat([X_train, flow_id_column, source_ip_column, dest_ip_column, timestamp_column, source_port_column, dest_port_column, protocol_column, flow_duration_column, total_fwd_packets_column, total_bwd_packets_column, total_fwd_length_column, total_bwd_length_column, fwd_pkt_len_max_column, fwd_pkt_len_min_column, fwd_pkt_len_mean_column, fwd_pkt_len_std_column, bwd_pkt_len_max_column, bwd_pkt_len_min_column, bwd_pkt_len_mean_column, bwd_pkt_len_std_column, flow_bytes_column, flow_packets_column, flow_iat_mean_column, flow_iat_std_column, flow_iat_max_column, flow_iat_min_column, fwd_iat_total_column, fwd_iat_mean_column, fwd_iat_std_column, fwd_iat_max_column, fwd_iat_min_column, bwd_iat_total_column, bwd_iat_mean_column, bwd_iat_std_column, bwd_iat_max_column, bwd_iat_min_column, fwd_psh_flags_column, bwd_psh_flags_column, fwd_urg_flags_column, bwd_urg_flags_column, fwd_header_length_column, bwd_header_length_column, fwd_packets_column, bwd_packets_column, min_pkt_length_column, max_pkt_length_column, pkt_length_mean_column, pkt_length_std_column, pkt_length_variance_column, fin_flag_count_column, syn_flag_count_column, rst_flag_count_column, psh_flag_count_column, ack_flag_count_column, urg_flag_count_column, cwe_flag_count_column, ece_flag_count_column, down_up_ratio_column, avg_pkt_size_column, avg_fwd_segment_column, avg_bwd_segment_column, fwd_avg_bytes_bulk_column, fwd_avg_packets_bulk_column, fwd_avg_bulk_rate_column, bwd_avg_bytes_bulk_column, bwd_avg_packets_bulk_column, bwd_avg_bulk_rate_column, subflow_fwd_packets_column, subflow_fwd_bytes_column, subflow_bwd_packets_column, subflow_bwd_bytes_column, init_win_bytes_fwd_column, init_win_bytes_bwd_column, act_data_pkt_fwd_column, min_seg_size_fwd_column, active_mean_column, active_std_column, active_max_column, active_min_column, idle_mean_column, idle_std_column, idle_max_column, idle_min_column], axis=1)\n",
        "# X_val = pd.concat([X_val, flow_id_column, source_ip_column, dest_ip_column, timestamp_column, source_port_column, dest_port_column, protocol_column, flow_duration_column, total_fwd_packets_column, total_bwd_packets_column, total_fwd_length_column, total_bwd_length_column, fwd_pkt_len_max_column, fwd_pkt_len_min_column, fwd_pkt_len_mean_column, fwd_pkt_len_std_column, bwd_pkt_len_max_column, bwd_pkt_len_min_column, bwd_pkt_len_mean_column, bwd_pkt_len_std_column, flow_bytes_column, flow_packets_column, flow_iat_mean_column, flow_iat_std_column, flow_iat_max_column, flow_iat_min_column, fwd_iat_total_column, fwd_iat_mean_column, fwd_iat_std_column, fwd_iat_max_column, fwd_iat_min_column, bwd_iat_total_column, bwd_iat_mean_column, bwd_iat_std_column, bwd_iat_max_column, bwd_iat_min_column, fwd_psh_flags_column, bwd_psh_flags_column, fwd_urg_flags_column, bwd_urg_flags_column, fwd_header_length_column, bwd_header_length_column, fwd_packets_column, bwd_packets_column, min_pkt_length_column, max_pkt_length_column, pkt_length_mean_column, pkt_length_std_column, pkt_length_variance_column, fin_flag_count_column, syn_flag_count_column, rst_flag_count_column, psh_flag_count_column, ack_flag_count_column, urg_flag_count_column, cwe_flag_count_column, ece_flag_count_column, down_up_ratio_column, avg_pkt_size_column, avg_fwd_segment_column, avg_bwd_segment_column, fwd_avg_bytes_bulk_column, fwd_avg_packets_bulk_column, fwd_avg_bulk_rate_column, bwd_avg_bytes_bulk_column, bwd_avg_packets_bulk_column, bwd_avg_bulk_rate_column, subflow_fwd_packets_column, subflow_fwd_bytes_column, subflow_bwd_packets_column, subflow_bwd_bytes_column, init_win_bytes_fwd_column, init_win_bytes_bwd_column, act_data_pkt_fwd_column, min_seg_size_fwd_column, active_mean_column, active_std_column, active_max_column, active_min_column, idle_mean_column, idle_std_column, idle_max_column, idle_min_column], axis=1)\n",
        "# X_test = pd.concat([X_test, flow_id_column, source_ip_column, dest_ip_column, timestamp_column, source_port_column, dest_port_column, protocol_column, flow_duration_column, total_fwd_packets_column, total_bwd_packets_column, total_fwd_length_column, total_bwd_length_column, fwd_pkt_len_max_column, fwd_pkt_len_min_column, fwd_pkt_len_mean_column, fwd_pkt_len_std_column, bwd_pkt_len_max_column, bwd_pkt_len_min_column, bwd_pkt_len_mean_column, bwd_pkt_len_std_column, flow_bytes_column, flow_packets_column, flow_iat_mean_column, flow_iat_std_column, flow_iat_max_column, flow_iat_min_column, fwd_iat_total_column, fwd_iat_mean_column, fwd_iat_std_column, fwd_iat_max_column, fwd_iat_min_column, bwd_iat_total_column, bwd_iat_mean_column, bwd_iat_std_column, bwd_iat_max_column, bwd_iat_min_column, fwd_psh_flags_column, bwd_psh_flags_column, fwd_urg_flags_column, bwd_urg_flags_column, fwd_header_length_column, bwd_header_length_column, fwd_packets_column, bwd_packets_column, min_pkt_length_column, max_pkt_length_column, pkt_length_mean_column, pkt_length_std_column, pkt_length_variance_column, fin_flag_count_column, syn_flag_count_column, rst_flag_count_column, psh_flag_count_column, ack_flag_count_column, urg_flag_count_column, cwe_flag_count_column, ece_flag_count_column, down_up_ratio_column, avg_pkt_size_column, avg_fwd_segment_column, avg_bwd_segment_column, fwd_avg_bytes_bulk_column, fwd_avg_packets_bulk_column, fwd_avg_bulk_rate_column, bwd_avg_bytes_bulk_column, bwd_avg_packets_bulk_column, bwd_avg_bulk_rate_column, subflow_fwd_packets_column, subflow_fwd_bytes_column, subflow_bwd_packets_column, subflow_bwd_bytes_column, init_win_bytes_fwd_column, init_win_bytes_bwd_column, act_data_pkt_fwd_column, min_seg_size_fwd_column, active_mean_column, active_std_column, active_max_column, active_min_column, idle_mean_column, idle_std_column, idle_max_column, idle_min_column], axis=1)\n",
        "\n",
        "# # Ensure the column order is correct\n",
        "# X_train = X_train[feature_cols]\n",
        "# X_val = X_val[feature_cols]\n",
        "# X_test = X_test[feature_cols]\n",
        "\n",
        "# y_train = to_categorical(y_train, num_classes=num_classes).astype('float32')\n",
        "# y_val = to_categorical(y_val, num_classes=num_classes).astype('float32')\n",
        "# y_test = to_categorical(y_test, num_classes=num_classes).astype('float32')\n",
        "\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_val = scaler.transform(X_val)\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "# input_shape = (X_train.shape[1],)\n",
        "\n",
        "# model = build_model(input_shape, num_classes)\n",
        "\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, min_lr=0.001)\n",
        "# model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=120, batch_size=64, callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model_path = '/content/drive/MyDrive/retrained_6G_model.pkl'\n",
        "# pickle.dump(model, open(model_path, 'wb'))  # Save the model using Pickle\n",
        "# print(f\"Trained model saved to {model_path}\")\n",
        "\n",
        "# y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "# y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# metrics = print_metrics(y_true, y_pred)  # Get the metrics as a string\n",
        "\n",
        "# # Save the metrics to a text file\n",
        "# metrics_path = '/content/drive/MyDrive/retrained_6G_model_metrics.txt'\n",
        "# with open(metrics_path, 'w') as f:\n",
        "#     f.write(metrics)\n",
        "\n",
        "# print(f\"Model metrics saved to {metrics_path}\")\n",
        "\n",
        "# del X_train, y_train, X_val, y_val, X_test, y_test, model\n",
        "# gc.collect()\n",
        "# import psutil\n",
        "# process = psutil.Process()\n",
        "# if process.memory_info().rss > 1000000000:\n",
        "#     print(\"Memory usage is high, exiting...\")\n",
        "#     exit(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import gc\n",
        "# import pickle\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "# from keras.models import Sequential, load_model\n",
        "# from keras.layers import Dense, Dropout, BatchNormalization\n",
        "# from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "# from keras.utils import to_categorical\n",
        "# from google.colab import drive\n",
        "\n",
        "# # Define a function to process datasets\n",
        "# def process_data(dataset, feature_cols, label_col):\n",
        "#     X = dataset[feature_cols]\n",
        "#     y = dataset[label_col]\n",
        "#     le = LabelEncoder()\n",
        "#     y = le.fit_transform(y)\n",
        "#     num_classes = len(le.classes_)\n",
        "#     y = to_categorical(y, num_classes=num_classes).astype('float32')\n",
        "#     return X, y, num_classes\n",
        "\n",
        "# # Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Function to print metrics\n",
        "# def print_metrics(y_true, y_pred):\n",
        "#     from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "#     accuracy = accuracy_score(y_true, y_pred)\n",
        "#     precision = precision_score(y_true, y_pred, average='weighted')\n",
        "#     recall = recall_score(y_true, y_pred, average='weighted')\n",
        "#     f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "#     confusion_mat = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "#     print(f\"Accuracy: {accuracy:.4f}\")\n",
        "#     print(f\"Precision: {precision:.4f}\")\n",
        "#     print(f\"Recall: {recall:.4f}\")\n",
        "#     print(f\"F1-Score: {f1:.4f}\")\n",
        "#     print(\"Confusion Matrix:\")\n",
        "#     print(confusion_mat)\n",
        "#     print()\n",
        "\n",
        "#     metrics = {\n",
        "#         'accuracy': accuracy,\n",
        "#         'precision': precision,\n",
        "#         'recall': recall,\n",
        "#         'f1_score': f1,\n",
        "#         'confusion_matrix': confusion_mat.tolist()\n",
        "#     }\n",
        "#     return metrics\n",
        "\n",
        "# # Define the model building function\n",
        "# def build_model(input_shape, num_classes):\n",
        "#     model = Sequential([\n",
        "#         Dense(128, activation='relu', input_shape=input_shape),\n",
        "#         BatchNormalization(),\n",
        "#         Dropout(0.2),\n",
        "#         Dense(64, activation='relu'),\n",
        "#         BatchNormalization(),\n",
        "#         Dropout(0.2),\n",
        "#         Dense(num_classes, activation='softmax')\n",
        "#     ])\n",
        "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# # Load the saved model\n",
        "# model_path = '/content/drive/MyDrive/trained_6G_model_label.h5'\n",
        "# model = load_model(model_path)\n",
        "\n",
        "# # Define paths and datasets\n",
        "# data_dir = '/content/drive/MyDrive/6data/CIC_IDS17/dataB/processed/'\n",
        "# datasets = [\"nsl_kdd.csv\", \"unsw_nb15.csv\"]  # Placeholder dataset names\n",
        "# label_cols = ['label', 'Label']\n",
        "# nsl_kdd_col_names = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\",\n",
        "#                      \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
        "#                      \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
        "#                      \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
        "#                      \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\"]\n",
        "# unsw_col_names = [\"srcip\", \"sport\", \"dstip\", \"dsport\", \"proto\", \"state\", \"dur\", \"sbytes\", \"dbytes\", \"sttl\", \"dttl\", \"sloss\", \"dloss\", \"service\", \"Sload\", \"Dload\", \"Spkts\",\n",
        "#                   \"Dpkts\", \"swin\", \"dwin\", \"stcpb\", \"dtcpb\", \"smeansz\", \"dmeansz\", \"trans_depth\", \"res_bdy_len\", \"Sjit\", \"Djit\", \"Stime\", \"Ltime\", \"Sintpkt\", \"Dintpkt\", \"tcprtt\",\n",
        "#                   \"synack\", \"ackdat\", \"is_sm_ips_ports\", \"ct_state_ttl\", \"ct_flw_http_mthd\", \"is_ftp_login\", \"ct_ftp_cmd\", \"ct_srv_src\", \"ct_srv_dst\", \"ct_dst_ltm\", \"ct_src_ltm\",\n",
        "#                   \"ct_src_dport_ltm\", \"ct_dst_sport_ltm\", \"ct_dst_src_ltm\", \"attack_cat\"]\n",
        "\n",
        "# # Set feature columns\n",
        "# feature_cols = [nsl_kdd_col_names, unsw_col_names[:-1]]  # Exclude the 'Label' column for UNSW-NB15\n",
        "\n",
        "# # Initialize batches\n",
        "# X_train_batches, y_train_batches = [], []\n",
        "# X_val_batches, y_val_batches = [], []\n",
        "# X_test_batches, y_test_batches = [], []\n",
        "\n",
        "# # Load and process each dataset\n",
        "# for dataset_name, label_col, feature_col in zip(datasets, label_cols, feature_cols):\n",
        "#     dataset_path = os.path.join(data_dir, dataset_name)\n",
        "#     dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "#     # Process the dataset\n",
        "#     X, y, num_classes = process_data(dataset, feature_col, label_col)\n",
        "\n",
        "#     # Split the dataset\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "#     X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
        "\n",
        "#     # Append to batches\n",
        "#     X_train_batches.append(X_train)\n",
        "#     y_train_batches.append(y_train)\n",
        "#     X_val_batches.append(X_val)\n",
        "#     y_val_batches.append(y_val)\n",
        "#     X_test_batches.append(X_test)\n",
        "#     y_test_batches.append(y_test)\n",
        "\n",
        "# # Concatenate all batches\n",
        "# X_train = pd.concat(X_train_batches, ignore_index=True)\n",
        "# y_train = np.concatenate(y_train_batches, axis=0)\n",
        "# X_val = pd.concat(X_val_batches, ignore_index=True)\n",
        "# y_val = np.concatenate(y_val_batches, axis=0)\n",
        "# X_test = pd.concat(X_test_batches, ignore_index=True)\n",
        "# y_test = np.concatenate(y_test_batches, axis=0)\n",
        "\n",
        "# # Save the datasets to Google Drive\n",
        "# dataset_path = '/content/drive/MyDrive/'\n",
        "# with open(os.path.join(dataset_path, 'X_train.pkl'), 'wb') as f:\n",
        "#     pickle.dump(X_train, f)\n",
        "# with open(os.path.join(dataset_path, 'y_train.pkl'), 'wb') as f:\n",
        "#     pickle.dump(y_train, f)\n",
        "# with open(os.path.join(dataset_path, 'X_val.pkl'), 'wb') as f:\n",
        "#     pickle.dump(X_val, f)\n",
        "# with open(os.path.join(dataset_path, 'y_val.pkl'), 'wb') as f:\n",
        "#     pickle.dump(y_val, f)\n",
        "# with open(os.path.join(dataset_path, 'X_test.pkl'), 'wb') as f:\n",
        "#     pickle.dump(X_test, f)\n",
        "# with open(os.path.join(dataset_path, 'y_test.pkl'), 'wb') as f:\n",
        "#     pickle.dump(y_test, f)\n",
        "\n",
        "# # Ensure the column order is correct for scaling\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_val = scaler.transform(X_val)\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "# input_shape = (X_train.shape[1],)\n",
        "# model = build_model(input_shape, num_classes)\n",
        "\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, min_lr=0.001)\n",
        "\n",
        "# model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=120, batch_size=64, callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "# # Save the fine-tuned model\n",
        "# fine_tuned_model_path = '/content/drive/MyDrive/retrained_and_finetuned_6G_model.h5'\n",
        "# model.save(fine_tuned_model_path)\n",
        "# print(f\"Trained model saved to {fine_tuned_model_path}\")\n",
        "\n",
        "# # Evaluate and print metrics\n",
        "# y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "# y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# metrics = print_metrics(y_true, y_pred)\n",
        "\n",
        "# # Save the metrics to a text file\n",
        "# metrics_path = '/content/drive/MyDrive/retrained_and_finetuned_6G_model_metrics.txt'\n",
        "# with open(metrics_path, 'w') as f:\n",
        "#     for key, value in metrics.items():\n",
        "#         f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "# print(f\"Model metrics saved to {metrics_path}\")\n",
        "\n",
        "# # Clear memory\n",
        "# del X_train, y_train, X_val, y_val, X_test, y_test, model\n",
        "# gc.collect()\n"
      ],
      "metadata": {
        "id": "e1PUO1fsD3dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizations"
      ],
      "metadata": {
        "id": "DHnj8Cqtx_qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load NSL-KDD dataset\n",
        "nsl_kdd_col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
        "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
        "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
        "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
        "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
        "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
        "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
        "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
        "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
        "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\"]\n",
        "\n",
        "nsl_kdd_train = pd.read_csv('/content/drive/MyDrive/6data/nsl-kdd/NSL_KDD_Train.csv', names=nsl_kdd_col_names, low_memory=False)\n",
        "nsl_kdd_test = pd.read_csv('/content/drive/MyDrive/6data/nsl-kdd/NSL_KDD_Test.csv', names=nsl_kdd_col_names, low_memory=False)\n",
        "\n",
        "# Load UNSW-NB15 dataset\n",
        "unsw_col_names = [\"srcip\", \"sport\", \"dstip\", \"dsport\", \"proto\", \"state\", \"dur\", \"sbytes\", \"dbytes\", \"sttl\", \"dttl\", \"sloss\", \"dloss\", \"service\", \"Sload\", \"Dload\", \"Spkts\",\n",
        "                  \"Dpkts\", \"swin\", \"dwin\", \"stcpb\", \"dtcpb\", \"smeansz\", \"dmeansz\", \"trans_depth\", \"res_bdy_len\", \"Sjit\", \"Djit\", \"Stime\", \"Ltime\", \"Sintpkt\", \"Dintpkt\", \"tcprtt\",\n",
        "                  \"synack\", \"ackdat\", \"is_sm_ips_ports\", \"ct_state_ttl\", \"ct_flw_http_mthd\", \"is_ftp_login\", \"ct_ftp_cmd\", \"ct_srv_src\", \"ct_srv_dst\", \"ct_dst_ltm\", \"ct_src_ltm\",\n",
        "                  \"ct_src_dport_ltm\", \"ct_dst_sport_ltm\", \"ct_dst_src_ltm\", \"attack_cat\", \"Label\"]\n",
        "\n",
        "\n",
        "unsw_frames = [pd.read_csv(f'/content/drive/MyDrive/6data/UNSW/UNSW-NB15_{i}.csv', names=unsw_col_names, low_memory=False) for i in range(1, 5)]\n",
        "unsw_nb15 = pd.concat(unsw_frames, ignore_index=True)\n",
        "\n",
        "# Handle mixed types by converting to appropriate types\n",
        "unsw_nb15['sport'] = pd.to_numeric(unsw_nb15['sport'], errors='coerce')\n",
        "unsw_nb15['dsport'] = pd.to_numeric(unsw_nb15['dsport'], errors='coerce')\n",
        "\n",
        "# Combine datasets\n",
        "combined_data = pd.concat([\n",
        "    nsl_kdd_train, nsl_kdd_test,\n",
        "    unsw_nb15\n",
        "], ignore_index=True)\n",
        "\n",
        "# Shuffle the combined dataset to ensure random distribution\n",
        "combined_data = combined_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Select a subset of the combined data if needed\n",
        "subset_size = 100000  # Choose an appropriate subset size to fit into memory\n",
        "combined_data = combined_data.sample(n=subset_size, random_state=42)\n",
        "\n",
        "# Features and labels (adjust based on actual dataset column names)\n",
        "X = combined_data.drop(columns=['label', 'Label', 'attack_cat', 'srcip', 'dstip'], errors='ignore')\n",
        "y = combined_data[['label', 'Label', 'attack_cat']].fillna('').astype(str).agg('-'.join, axis=1)\n",
        "\n",
        "# Fill missing values in numerical columns\n",
        "for col in X.select_dtypes(include=['float64', 'int64']).columns:\n",
        "    X[col] = X[col].fillna(0)\n",
        "\n",
        "# Encode categorical variables and standardize numerical features\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "numeric_features = X.select_dtypes(exclude=['object']).columns\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)])\n",
        "\n",
        "# Create preprocessing and training pipeline\n",
        "model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                 ('classifier', SVC(probability=True))])\n",
        "\n",
        "# Encode labels\n",
        "lb = LabelBinarizer()\n",
        "y = lb.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"SVM\": SVC(probability=True),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"k-NN\": KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "# Plotting setup\n",
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "# Colors for ROC curves\n",
        "colors = [\"blue\", \"green\", \"red\", \"orange\"]\n",
        "\n",
        "for i, (name, model) in enumerate(models.items()):\n",
        "    # Create pipeline for each model\n",
        "    model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                     ('classifier', model)])\n",
        "    # Train the model\n",
        "    model_pipeline.fit(X_train, np.argmax(y_train, axis=1))\n",
        "\n",
        "    # Predict and compute metrics\n",
        "    y_pred = model_pipeline.predict(X_val)\n",
        "    y_pred_prob = model_pipeline.predict_proba(X_val) if hasattr(model_pipeline, \"predict_proba\") else model_pipeline.decision_function(X_val)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(np.argmax(y_val, axis=1), y_pred)\n",
        "    plt.subplot(2, 4, i + 1)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "    plt.title(f\"Confusion Matrix for {name}\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "\n",
        "    # ROC Curve and AUC\n",
        "    if y_val.shape[1] > 2:  # Multi-class\n",
        "        lb = LabelBinarizer()\n",
        "        y_val_bin = lb.fit_transform(np.argmax(y_val, axis=1))\n",
        "        if hasattr(model_pipeline, \"predict_proba\"):\n",
        "            y_score = model_pipeline.predict_proba(X_val)\n",
        "        else:\n",
        "            y_score = model_pipeline.decision_function(X_val)\n",
        "\n",
        "\n",
        "\n",
        "        fpr = {}\n",
        "        tpr = {}\n",
        "        roc_auc = {}\n",
        "        for j in range(y_val.shape[1]):\n",
        "            fpr[j], tpr[j], _ = roc_curve(y_val_bin[:, j], y_score[:, j])\n",
        "            roc_auc[j] = auc(fpr[j], tpr[j])\n",
        "\n",
        "\n",
        "        plt.subplot(2, 4, i + 5)\n",
        "        for j in range(y_val.shape[1]):\n",
        "            plt.plot(fpr[j], tpr[j], label=f'Class {j} (AUC = {roc_auc[j]:.2f})', color=colors[j % len(colors)])\n",
        "        plt.plot([0, 1], [0, 1], \"k--\")\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.title(f\"ROC Curve for {name}\")\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate\")\n",
        "        plt.legend(loc=\"lower right\")\n",
        "\n",
        "    else:\n",
        "        fpr, tpr, _ = roc_curve(np.argmax(y_val, axis=1), y_pred_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.subplot(2, 4, i + 5)\n",
        "        plt.plot(fpr, tpr, color=colors[i], lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], \"k--\", lw=2)\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.title(f\"ROC Curve for {name}\")\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate\")\n",
        "        plt.legend(loc=\"lower right\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score\n",
        "# from sklearn.svm import SVC\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler, LabelBinarizer, OneHotEncoder\n",
        "\n",
        "# # Load NSL-KDD dataset\n",
        "# nsl_kdd_col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
        "#     \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
        "#     \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
        "#     \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
        "#     \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
        "#     \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
        "#     \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
        "#     \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
        "#     \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
        "#     \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\"]\n",
        "\n",
        "# nsl_kdd_train = pd.read_csv('/content/drive/MyDrive/6data/nsl-kdd/NSL_KDD_Train.csv', names=nsl_kdd_col_names)\n",
        "# nsl_kdd_test = pd.read_csv('/content/drive/MyDrive/6data/nsl-kdd/NSL_KDD_Test.csv', names=nsl_kdd_col_names)\n",
        "\n",
        "# # Load UNSW-NB15 dataset\n",
        "# unsw_col_names = [\"srcip\", \"sport\", \"dstip\", \"dsport\", \"proto\", \"state\", \"dur\", \"sbytes\", \"dbytes\", \"sttl\", \"dttl\", \"sloss\", \"dloss\", \"service\", \"Sload\", \"Dload\", \"Spkts\",\n",
        "#                   \"Dpkts\", \"swin\", \"dwin\", \"stcpb\", \"dtcpb\", \"smeansz\", \"dmeansz\", \"trans_depth\", \"res_bdy_len\", \"Sjit\", \"Djit\", \"Stime\", \"Ltime\", \"Sintpkt\", \"Dintpkt\", \"tcprtt\",\n",
        "#                   \"synack\", \"ackdat\", \"is_sm_ips_ports\", \"ct_state_ttl\", \"ct_flw_http_mthd\", \"is_ftp_login\", \"ct_ftp_cmd\", \"ct_srv_src\", \"ct_srv_dst\", \"ct_dst_ltm\", \"ct_src_ltm\",\n",
        "#                   \"ct_src_dport_ltm\", \"ct_dst_sport_ltm\", \"ct_dst_src_ltm\", \"attack_cat\", \"Label\"]\n",
        "# unsw_frames = [pd.read_csv(f'/content/drive/MyDrive/6data/UNSW/UNSW-NB15_{i}.csv', names=unsw_col_names, low_memory=False) for i in range(1, 5)]\n",
        "# unsw_nb15 = pd.concat(unsw_frames, ignore_index=True)\n",
        "\n",
        "# # Handle mixed types by converting to appropriate types\n",
        "# unsw_nb15['sport'] = pd.to_numeric(unsw_nb15['sport'], errors='coerce')\n",
        "# unsw_nb15['dsport'] = pd.to_numeric(unsw_nb15['dsport'], errors='coerce')\n",
        "\n",
        "# # Combine datasets\n",
        "# combined_data = pd.concat([\n",
        "#     nsl_kdd_train, nsl_kdd_test,\n",
        "#     unsw_nb15\n",
        "# ], ignore_index=True)\n",
        "\n",
        "# # Features and labels (adjust based on actual dataset column names)\n",
        "# X = combined_data.drop(columns=['label', 'Label', 'attack_cat', 'srcip', 'dstip', 'proto', 'state'], errors='ignore')\n",
        "\n",
        "# # Fill missing values\n",
        "# X = X.fillna(0)\n",
        "\n",
        "# # Handle label encoding consistently\n",
        "# combined_data['combined_label'] = combined_data[['label', 'Label', 'attack_cat']].fillna('').astype(str).agg('-'.join, axis=1)\n",
        "# y = combined_data['combined_label']\n",
        "\n",
        "# # Encode labels\n",
        "# lb = LabelBinarizer()\n",
        "# y = lb.fit_transform(y)\n",
        "\n",
        "# # Standardize the data\n",
        "# scaler = StandardScaler()\n",
        "# X = scaler.fit_transform(X)\n",
        "\n",
        "# # Split data\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Initialize models\n",
        "# models = {\n",
        "#     \"SVM\": SVC(probability=True),\n",
        "#     \"Random Forest\": RandomForestClassifier(),\n",
        "#     \"Decision Tree\": DecisionTreeClassifier(),\n",
        "#     \"k-NN\": KNeighborsClassifier()\n",
        "# }\n",
        "\n",
        "# # Plotting setup\n",
        "# plt.figure(figsize=(18, 12))\n",
        "\n",
        "# # Colors for ROC curves\n",
        "# colors = [\"blue\", \"green\", \"red\", \"orange\"]\n",
        "\n",
        "# for i, (name, model) in enumerate(models.items()):\n",
        "#     # Train the model\n",
        "#     model.fit(X_train, np.argmax(y_train, axis=1))\n",
        "\n",
        "#     # Predict and compute metrics\n",
        "#     y_pred = model.predict(X_val)\n",
        "#     y_pred_prob = model.predict_proba(X_val) if hasattr(model, \"predict_proba\") else model.decision_function(X_val)\n",
        "\n",
        "#     # Confusion Matrix\n",
        "#     cm = confusion_matrix(np.argmax(y_val, axis=1), y_pred)\n",
        "#     plt.subplot(2, 4, i + 1)\n",
        "#     sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "#     plt.title(f\"Confusion Matrix for {name}\")\n",
        "#     plt.xlabel(\"Predicted\")\n",
        "#     plt.ylabel(\"Actual\")\n",
        "\n",
        "#     # ROC Curve and AUC\n",
        "#     if y_val.shape[1] > 2:  # Multi-class\n",
        "#         lb = LabelBinarizer()\n",
        "#         y_val_bin = lb.fit_transform(np.argmax(y_val, axis=1))\n",
        "#         if hasattr(model, \"predict_proba\"):\n",
        "#             y_score = model.predict_proba(X_val)\n",
        "#         else:\n",
        "#             y_score = model.decision_function(X_val)\n",
        "\n",
        "#         # Compute ROC curve and AUC for each class\n",
        "#         fpr = {}\n",
        "#         tpr = {}\n",
        "#         roc_auc = {}\n",
        "#         for j in range(y_val.shape[1]):\n",
        "#             fpr[j], tpr[j], _ = roc_curve(y_val_bin[:, j], y_score[:, j])\n",
        "#             roc_auc[j] = auc(fpr[j], tpr[j])\n",
        "\n",
        "#         # Plot ROC curve for each class\n",
        "#         plt.subplot(2, 4, i + 5)\n",
        "#         for j in range(y_val.shape[1]):\n",
        "#             plt.plot(fpr[j], tpr[j], label=f'Class {j} (AUC = {roc_auc[j]:.2f})', color=colors[j % len(colors)])\n",
        "#         plt.plot([0, 1], [0, 1], \"k--\")\n",
        "#         plt.xlim([0.0, 1.0])\n",
        "#         plt.ylim([0.0, 1.05])\n",
        "#         plt.title(f\"ROC Curve for {name}\")\n",
        "#         plt.xlabel(\"False Positive Rate\")\n",
        "#         plt.ylabel(\"True Positive Rate\")\n",
        "#         plt.legend(loc=\"lower right\")\n",
        "#     else:  # Binary\n",
        "#         fpr, tpr, _ = roc_curve(np.argmax(y_val, axis=1), y_pred_prob)\n",
        "#         roc_auc = auc(fpr, tpr)\n",
        "#         plt.subplot(2, 4, i + 5)\n",
        "#         plt.plot(fpr, tpr, color=colors[i], lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "#         plt.plot([0, 1], [0, 1], \"k--\", lw=2)\n",
        "#         plt.xlim([0.0, 1.0])\n",
        "#         plt.ylim([0.0, 1.05])\n",
        "#         plt.title(f\"ROC Curve for {name}\")\n",
        "#         plt.xlabel(\"False Positive Rate\")\n",
        "#         plt.ylabel(\"True Positive Rate\")\n",
        "#         plt.legend(loc=\"lower right\")\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "UtNhe8172B1S",
        "outputId": "4e13c0b5-3eb1-4a72-8e71-02f3e1eca994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/6data/nsl-kdd/NSL_KDD_Train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3247630a8115>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\"]\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mnsl_kdd_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/6data/nsl-kdd/NSL_KDD_Train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnsl_kdd_col_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mnsl_kdd_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/6data/nsl-kdd/NSL_KDD_Test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnsl_kdd_col_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/6data/nsl-kdd/NSL_KDD_Train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvgVpPBjC5F6"
      },
      "source": [
        "## MileStone 3: Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize"
      ],
      "metadata": {
        "id": "_DOdgD7UZ8r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.utils import to_categorical\n",
        "from google.colab import drive\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Function to print metrics\n",
        "def print_metrics(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    confusion_mat = confusion_matrix(y_true, y_pred)\n",
        "    auc = roc_auc_score(y_true, y_pred, multi_class='ovr')\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(f\"AUC: {auc:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_mat)\n",
        "    print()\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'auc': auc,\n",
        "        'confusion_matrix': confusion_mat.tolist()\n",
        "    }\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "ZgjMo4ov6Ree"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the model building function\n"
      ],
      "metadata": {
        "id": "pM1jo9Cm6a2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Function to process datasets\n",
        "def process_data(dataset, feature_cols, label_col):\n",
        "    X = dataset[feature_cols]\n",
        "    y = dataset[label_col]\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y)\n",
        "    num_classes = len(le.classes_)\n",
        "    y = to_categorical(y, num_classes=num_classes).astype('float32')\n",
        "    return X, y, num_classes"
      ],
      "metadata": {
        "id": "4MGpSrPw6Ysk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Define paths and datasets"
      ],
      "metadata": {
        "id": "iV5T3uNW6iGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/6data/'\n",
        "datasets = [nsl_kdd, unsw_nb15]\n",
        "label_cols = ['label', 'Label']\n",
        "unsw_col_names = [\"srcip\", \"sport\", \"dstip\", \"dsport\", \"proto\", \"state\", \"dur\", \"sbytes\", \"dbytes\", \"sttl\", \"dttl\", \"sloss\", \"dloss\", \"service\", \"Sload\", \"Dload\", \"Spkts\",\n",
        "                  \"Dpkts\", \"swin\", \"dwin\", \"stcpb\", \"dtcpb\", \"smeansz\", \"dmeansz\", \"trans_depth\", \"res_bdy_len\", \"Sjit\", \"Djit\", \"Stime\", \"Ltime\", \"Sintpkt\", \"Dintpkt\", \"tcprtt\",\n",
        "                  \"synack\", \"ackdat\", \"is_sm_ips_ports\", \"ct_state_ttl\", \"ct_flw_http_mthd\", \"is_ftp_login\", \"ct_ftp_cmd\", \"ct_srv_src\", \"ct_srv_dst\", \"ct_dst_ltm\", \"ct_src_ltm\",\n",
        "                  \"ct_src_dport_ltm\", \"ct_dst_sport_ltm\", \"ct_dst_src_ltm\", \"attack_cat\"]\n",
        "nsl_kdd_col_names = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\",\n",
        "                     \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
        "                     \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
        "                     \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
        "                     \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\"]\n",
        "\n",
        "\n",
        "# Feature columns\n",
        "feature_cols_list = [nsl_kdd_col_names, unsw_col_names[:-1]]\n",
        "\n",
        "# Initialize batches\n",
        "X_train_batches, y_train_batches = [], []\n",
        "X_val_batches, y_val_batches = [], []\n",
        "X_test_batches, y_test_batches = [], []\n",
        "\n"
      ],
      "metadata": {
        "id": "P-hCJy6XPoVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and process each dataset\n",
        "for dataset, label_col, feature_col in zip(datasets, label_cols, feature_cols_list):\n",
        "    # Process the dataset\n",
        "    X, y = dataset[feature_col], dataset[label_col]\n",
        "\n",
        "    # Encode the target variable\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y)\n",
        "\n",
        "\n",
        "# for dataset_name, label_col, feature_col in zip(datasets, label_cols, feature_cols_list):\n",
        "#     dataset_path = os.path.join(data_dir, dataset_name)\n",
        "#     dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "#     # Process the dataset\n",
        "#     X, y, num_classes = process_data(dataset, feature_col, label_col)\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
        "\n",
        "    # Append to batches\n",
        "    X_train_batches.append(X_train)\n",
        "    y_train_batches.append(y_train)\n",
        "    X_val_batches.append(X_val)\n",
        "    y_val_batches.append(y_val)\n",
        "    X_test_batches.append(X_test)\n",
        "    y_test_batches.append(y_test)\n",
        "\n",
        "# Concatenate all batches\n",
        "X_train = pd.concat(X_train_batches, ignore_index=True)\n",
        "y_train = np.concatenate(y_train_batches, axis=0)\n",
        "X_val = pd.concat(X_val_batches, ignore_index=True)\n",
        "y_val = np.concatenate(y_val_batches, axis=0)\n",
        "X_test = pd.concat(X_test_batches, ignore_index=True)\n",
        "y_test = np.concatenate(y_test_batches, axis=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "Jc3HEhQ-64R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the datasets to Google Drive"
      ],
      "metadata": {
        "id": "aS6o8Haq9MsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset_path = '/content/drive/MyDrive/'\n",
        "with open(os.path.join(dataset_path, 'X_train.pkl'), 'wb') as f:\n",
        "    pickle.dump(X_train, f)\n",
        "with open(os.path.join(dataset_path, 'y_train.pkl'), 'wb') as f:\n",
        "    pickle.dump(y_train, f)\n",
        "with open(os.path.join(dataset_path, 'X_val.pkl'), 'wb') as f:\n",
        "    pickle.dump(X_val, f)\n",
        "with open(os.path.join(dataset_path, 'y_val.pkl'), 'wb') as f:\n",
        "    pickle.dump(y_val, f)\n",
        "with open(os.path.join(dataset_path, 'X_test.pkl'), 'wb') as f:\n",
        "    pickle.dump(X_test, f)\n",
        "with open(os.path.join(dataset_path, 'y_test.pkl'), 'wb') as f:\n",
        "    pickle.dump(y_test, f)\n"
      ],
      "metadata": {
        "id": "0r0ugMtqee-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardize the features"
      ],
      "metadata": {
        "id": "OD7radiqejoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dask dask-ml scikit-optimize"
      ],
      "metadata": {
        "id": "n-C5U2rkMcI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "from dask_ml.preprocessing import Categorizer, DummyEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from skopt import BayesSearchCV\n",
        "from scipy.sparse import csr_matrix\n",
        "import os\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/6data/'\n",
        "dataset_dirs = ['nsl-kdd', 'UNSW']\n",
        "label_cols = ['label', 'Label']\n",
        "\n",
        "# Define column names\n",
        "unsw_col_names = [\"srcip\", \"sport\", \"dstip\", \"dsport\", \"proto\", \"state\", \"dur\", \"sbytes\", \"dbytes\", \"sttl\", \"dttl\", \"sloss\", \"dloss\", \"service\", \"Sload\", \"Dload\", \"Spkts\",\n",
        "                  \"Dpkts\", \"swin\", \"dwin\", \"stcpb\", \"dtcpb\", \"smeansz\", \"dmeansz\", \"trans_depth\", \"res_bdy_len\", \"Sjit\", \"Djit\", \"Stime\", \"Ltime\", \"Sintpkt\", \"Dintpkt\", \"tcprtt\",\n",
        "                  \"synack\", \"ackdat\", \"is_sm_ips_ports\", \"ct_state_ttl\", \"ct_flw_http_mthd\", \"is_ftp_login\", \"ct_ftp_cmd\", \"ct_srv_src\", \"ct_srv_dst\", \"ct_dst_ltm\", \"ct_src_ltm\",\n",
        "                  \"ct_src_dport_ltm\", \"ct_dst_sport_ltm\", \"ct_dst_src_ltm\", \"attack_cat\"]\n",
        "nsl_kdd_col_names = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\",\n",
        "                     \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
        "                     \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
        "                     \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
        "                     \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\"]\n",
        "\n",
        "# Feature columns list\n",
        "feature_cols_list = [nsl_kdd_col_names, unsw_col_names[:-1]]\n",
        "\n",
        "# Initialize batches\n",
        "X_train_batches, y_train_batches = [], []\n",
        "X_val_batches, y_val_batches = [], []\n",
        "X_test_batches, y_test_batches = [], []\n",
        "\n",
        "# Load and process each dataset\n",
        "for dataset_dir, label_col, feature_cols in zip(dataset_dirs, label_cols, feature_cols_list):\n",
        "    dataset_path = os.path.join(data_dir, dataset_dir, \"*.csv\")\n",
        "    try:\n",
        "        dataset = dd.read_csv(dataset_path)\n",
        "        print(f\"Columns in {dataset_dir}: {dataset.columns}\")\n",
        "    except OSError:\n",
        "        print(f\"Error: No files found for dataset path: {dataset_path}\")\n",
        "        continue\n",
        "\n",
        "    # Verify feature columns\n",
        "    missing_cols = [col for col in feature_cols if col not in dataset.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"Missing columns in {dataset_dir}: {missing_cols}\")\n",
        "        continue\n",
        "\n",
        "    # Split data\n",
        "    X = dataset[feature_cols]\n",
        "    y = dataset[label_col]\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_test, y_train, y_test = dd.compute(*dd.train_test_split(X, y, test_size=0.2, random_state=42))\n",
        "    X_train, X_val, y_train, y_val = dd.compute(*dd.train_test_split(X_train, y_train, test_size=0.25, random_state=42))\n",
        "\n",
        "    # Append to batches\n",
        "    X_train_batches.append(X_train)\n",
        "    y_train_batches.append(y_train)\n",
        "    X_val_batches.append(X_val)\n",
        "    y_val_batches.append(y_val)\n",
        "    X_test_batches.append(X_test)\n",
        "    y_test_batches.append(y_test)\n",
        "\n",
        "# Concatenate all batches\n",
        "if X_train_batches and y_train_batches and X_val_batches and y_val_batches and X_test_batches and y_test_batches:\n",
        "    X_train = dd.concat(X_train_batches).compute()\n",
        "    y_train = dd.concat(y_train_batches).compute()\n",
        "    X_val = dd.concat(X_val_batches).compute()\n",
        "    y_val = dd.concat(y_val_batches).compute()\n",
        "    X_test = dd.concat(X_test_batches).compute()\n",
        "    y_test = dd.concat(y_test_batches).compute()\n",
        "else:\n",
        "    print(\"No data to process. Please check the dataset paths and column names.\")\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols = ['protocol_type', 'service', 'flag', 'proto', 'state']\n",
        "\n",
        "# One-hot encode categorical columns\n",
        "X_train = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)\n",
        "X_val = pd.get_dummies(X_val, columns=categorical_cols, drop_first=True)\n",
        "X_test = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Get the union of columns\n",
        "all_columns = list(set(X_train.columns) | set(X_val.columns) | set(X_test.columns))\n",
        "\n",
        "# Reindex the DataFrames to have the same columns\n",
        "X_train = X_train.reindex(columns=all_columns, fill_value=0)\n",
        "X_val = X_val.reindex(columns=all_columns, fill_value=0)\n",
        "X_test = X_test.reindex(columns=all_columns, fill_value=0)\n",
        "\n",
        "# Convert to sparse matrices\n",
        "X_train = csr_matrix(X_train.values)\n",
        "X_val = csr_matrix(X_val.values)\n",
        "X_test = csr_matrix(X_test.values)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler(with_mean=False)  # with_mean=False because sparse matrices do not support mean centering\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Downsample the dataset for hyperparameter tuning to avoid memory issues\n",
        "sample_size = min(10000, X_train.shape[0])  # Use 10,000 samples or less if fewer samples are available\n",
        "X_train_sample = X_train[:sample_size]\n",
        "y_train_sample = y_train[:sample_size]\n",
        "\n",
        "# Hyperparameter tuning for SVM using Bayesian Optimization\n",
        "svm_model = SVC(probability=True)\n",
        "params = {\n",
        "    'C': (1e-6, 1e+6, 'log-uniform'),\n",
        "    'gamma': (1e-6, 1e+1, 'log-uniform'),\n",
        "    'degree': (1, 8),\n",
        "    'kernel': ['linear', 'poly', 'rbf']\n",
        "}\n",
        "\n",
        "opt = BayesSearchCV(svm_model, params, n_iter=32, cv=3, n_jobs=-1, scoring='accuracy')\n",
        "opt.fit(X_train_sample, y_train_sample)\n",
        "\n",
        "# Best SVM model\n",
        "best_svm = opt.best_estimator_\n",
        "\n",
        "# Evaluate on the full validation set\n",
        "val_accuracy = best_svm.score(X_val, y_val)\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate on the full test set\n",
        "test_accuracy = best_svm.score(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import dask.dataframe as dd\n",
        "from dask_ml.preprocessing import Categorizer, DummyEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from skopt import BayesSearchCV\n",
        "from scipy.sparse import csr_matrix\n",
        "import os\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/6data/'\n",
        "dataset_dirs = ['nsl-kdd', 'UNSW']\n",
        "label_cols = ['label', 'Label']\n",
        "\n",
        "# Define column names\n",
        "unsw_col_names = [\"srcip\", \"sport\", \"dstip\", \"dsport\", \"proto\", \"state\", \"dur\", \"sbytes\", \"dbytes\", \"sttl\", \"dttl\", \"sloss\", \"dloss\", \"service\", \"Sload\", \"Dload\", \"Spkts\",\n",
        "                  \"Dpkts\", \"swin\", \"dwin\", \"stcpb\", \"dtcpb\", \"smeansz\", \"dmeansz\", \"trans_depth\", \"res_bdy_len\", \"Sjit\", \"Djit\", \"Stime\", \"Ltime\", \"Sintpkt\", \"Dintpkt\", \"tcprtt\",\n",
        "                  \"synack\", \"ackdat\", \"is_sm_ips_ports\", \"ct_state_ttl\", \"ct_flw_http_mthd\", \"is_ftp_login\", \"ct_ftp_cmd\", \"ct_srv_src\", \"ct_srv_dst\", \"ct_dst_ltm\", \"ct_src_ltm\",\n",
        "                  \"ct_src_dport_ltm\", \"ct_dst_sport_ltm\", \"ct_dst_src_ltm\", \"attack_cat\"]\n",
        "nsl_kdd_col_names = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\",\n",
        "                     \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
        "                     \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
        "                     \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
        "                     \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\"]\n",
        "\n",
        "# Feature columns list\n",
        "feature_cols_list = [nsl_kdd_col_names, unsw_col_names[:-1]]\n",
        "\n",
        "# Initialize batches\n",
        "X_train_batches, y_train_batches = [], []\n",
        "X_val_batches, y_val_batches = [], []\n",
        "X_test_batches, y_test_batches = [], []\n",
        "\n",
        "# Load and process each dataset\n",
        "for dataset_dir, label_col, feature_cols in zip(dataset_dirs, label_cols, feature_cols_list):\n",
        "    dataset_path = os.path.join(data_dir, dataset_dir, \"*.csv\")\n",
        "    try:\n",
        "        dataset = dd.read_csv(dataset_path)\n",
        "        print(f\"Columns in {dataset_dir}: {dataset.columns}\")\n",
        "    except OSError:\n",
        "        print(f\"Error: No files found for dataset path: {dataset_path}\")\n",
        "        continue\n",
        "\n",
        "    # Verify feature columns\n",
        "    missing_cols = [col for col in feature_cols if col not in dataset.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"Missing columns in {dataset_dir}: {missing_cols}\")\n",
        "        continue\n",
        "\n",
        "    # Split data\n",
        "    X = dataset[feature_cols]\n",
        "    y = dataset[label_col]\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_test, y_train, y_test = dd.compute(*dd.train_test_split(X, y, test_size=0.2, random_state=42))\n",
        "    X_train, X_val, y_train, y_val = dd.compute(*dd.train_test_split(X_train, y_train, test_size=0.25, random_state=42))\n",
        "\n",
        "    # Append to batches\n",
        "    X_train_batches.append(X_train)\n",
        "    y_train_batches.append(y_train)\n",
        "    X_val_batches.append(X_val)\n",
        "    y_val_batches.append(y_val)\n",
        "    X_test_batches.append(X_test)\n",
        "    y_test_batches.append(y_test)\n",
        "\n",
        "# Concatenate all batches\n",
        "if X_train_batches and y_train_batches and X_val_batches and y_val_batches and X_test_batches and y_test_batches:\n",
        "    X_train = dd.concat(X_train_batches).compute()\n",
        "    y_train = dd.concat(y_train_batches).compute()\n",
        "    X_val = dd.concat(X_val_batches).compute()\n",
        "    y_val = dd.concat(y_val_batches).compute()\n",
        "    X_test = dd.concat(X_test_batches).compute()\n",
        "    y_test = dd.concat(y_test_batches).compute()\n",
        "else:\n",
        "    print(\"No data to process. Please check the dataset paths and column names.\")\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols = ['protocol_type', 'service', 'flag', 'proto', 'state']\n",
        "\n",
        "# One-hot encode categorical columns\n",
        "X_train = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)\n",
        "X_val = pd.get_dummies(X_val, columns=categorical_cols, drop_first=True)\n",
        "X_test = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Get the union of columns\n",
        "all_columns = list(set(X_train.columns) | set(X_val.columns) | set(X_test.columns))\n",
        "\n",
        "# Reindex the DataFrames to have the same columns\n",
        "X_train = X_train.reindex(columns=all_columns, fill_value=0)\n",
        "X_val = X_val.reindex(columns=all_columns, fill_value=0)\n",
        "X_test = X_test.reindex(columns=all_columns, fill_value=0)\n",
        "\n",
        "# Convert to sparse matrices\n",
        "X_train = csr_matrix(X_train.values)\n",
        "X_val = csr_matrix(X_val.values)\n",
        "X_test = csr_matrix(X_test.values)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler(with_mean=False)  # with_mean=False because sparse matrices do not support mean centering\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Downsample the dataset for hyperparameter tuning to avoid memory issues\n",
        "sample_size = min(10000, X_train.shape[0])  # Use 10,000 samples or less if fewer samples are available\n",
        "X_train_sample = X_train[:sample_size]\n",
        "y_train_sample = y_train[:sample_size]\n",
        "\n",
        "# Hyperparameter tuning for SVM using Bayesian Optimization\n",
        "svm_model = SVC(probability=True)\n",
        "params = {\n",
        "    'C': (1e-6, 1e+6, 'log-uniform'),\n",
        "    'gamma': (1e-6, 1e+1, 'log-uniform'),\n",
        "    'degree': (1, 8),\n",
        "    'kernel': ['linear', 'poly', 'rbf']\n",
        "}\n",
        "\n",
        "opt = BayesSearchCV(svm_model, params, n_iter=32, cv=3, n_jobs=-1, scoring='accuracy')\n",
        "opt.fit(X_train_sample, y_train_sample)\n",
        "\n",
        "# Best SVM model\n",
        "best_svm = opt.best_estimator_\n",
        "\n",
        "# Evaluate on the full validation set\n",
        "val_accuracy = best_svm.score(X_val, y_val)\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate on the full test set\n",
        "test_accuracy = best_svm.score(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NIOQmCXMP1TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define other models\n",
        "rf_model = RandomForestClassifier()\n",
        "dt_model = DecisionTreeClassifier()\n",
        "knn_model = KNeighborsClassifier()\n",
        "\n",
        "# Ensemble with voting classifier\n",
        "ensemble_model = VotingClassifier(estimators=[\n",
        "    ('svm', best_svm),\n",
        "    ('rf', rf_model),\n",
        "    ('dt', dt_model),\n",
        "    ('knn', knn_model)\n",
        "], voting='soft')\n",
        "\n",
        "# Train ensemble model\n",
        "ensemble_model.fit(X_train, np.argmax(y_train, axis=1))\n",
        "\n",
        "# Evaluate and print metrics for ensemble model\n",
        "y_pred = ensemble_model.predict(X_test)\n",
        "metrics = print_metrics(np.argmax(y_test, axis=1), y_pred)\n",
        "\n",
        "# Save the fine-tuned model using Pickle\n",
        "fine_tuned_model_path = '/content/drive/MyDrive/retrained_and_finetuned_6G_model.pkl'\n",
        "with open(fine_tuned_model_path, 'wb') as f:\n",
        "    pickle.dump(ensemble_model, f)\n",
        "print(f\"Trained model saved to {fine_tuned_model_path}\")\n",
        "\n",
        "# Save the metrics to a text file\n",
        "metrics_path = '/content/drive/MyDrive/retrained_and_finetuned_6G_model_metrics.txt'\n",
        "with open(metrics_path, 'w') as f:\n",
        "    for key, value in metrics.items():\n",
        "        f.write(f\"{key}: {value}\\n\")\n",
        "print(f\"Model metrics saved to {metrics_path}\")\n",
        "\n",
        "# Clear memory\n",
        "del X_train, y_train, X_val, y_val, X_test, y_test, ensemble_model\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "Ope2BlzphVVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Milestone 4\n",
        "\n",
        "\n",
        "<p>\n",
        "This stage allows us to thoroughly assess whether these optimized models effectively address the unique security and privacy challenges of 6G networks. During this phase, we will extensively validate the models using both holdout and k-fold cross-validation techniques to reliably measure their performance on new, unseen data. We will use various evaluation metrics—such as accuracy, precision, recall, F1 score, and AUC-ROC—to thoroughly assess how well the models classify different types of cyber threats. We will subject the models to stress tests under simulated attack conditions to evaluate their resilience and response capabilities. The insights from this evaluation will guide the final adjustments and optimizations, preparing the models for deployment in live 6G networks.\n",
        "  \n",
        "</p>"
      ],
      "metadata": {
        "id": "1GKeNfdby4mO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bUSz4hxq_TAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Function to generate synthetic data\n",
        "def generate_synthetic_data(model, num_samples=10000, num_timesteps=1, num_features=78):\n",
        "    np.random.seed(42)\n",
        "    noise = np.random.normal(size=(num_samples, num_timesteps, num_features))\n",
        "    synthetic_data = model.predict(noise)\n",
        "    return noise, synthetic_data\n",
        "\n",
        "# Load LSTM model and generate synthetic data\n",
        "lstm_model = load_model('/content/drive/MyDrive/lstm_best_model.h5')\n",
        "num_samples = 10000\n",
        "num_timesteps = 1  # Match the expected timesteps\n",
        "num_features = 78  # Match the expected features\n",
        "X_synthetic, y_synthetic = generate_synthetic_data(lstm_model, num_samples, num_timesteps, num_features)\n",
        "\n",
        "# Ensure correct shapes and sizes\n",
        "X_synthetic = X_synthetic.reshape(num_samples, num_features)  # Flatten for CSV saving\n",
        "y_synthetic = y_synthetic.reshape(num_samples, -1)  # Ensure 2D shape for saving\n",
        "\n",
        "# Save synthetic data\n",
        "synthetic_data_path = '/content/drive/MyDrive/generated_data.csv'\n",
        "pd.DataFrame(X_synthetic).to_csv(synthetic_data_path, index=False)\n",
        "pd.DataFrame(y_synthetic).to_csv(synthetic_data_path.replace('.csv', '_labels.csv'), index=False)\n",
        "\n",
        "print(f\"Synthetic data saved to {synthetic_data_path}\")\n",
        "\n",
        "# Load the existing 6G model\n",
        "model_6G = load_model('/content/drive/MyDrive/trained_6G_model_label.h5')\n",
        "\n",
        "# Load synthetic data\n",
        "X_synthetic = pd.read_csv(synthetic_data_path).values\n",
        "y_synthetic = pd.read_csv(synthetic_data_path.replace('.csv', '_labels.csv')).values\n",
        "\n",
        "# One-hot encode labels if necessary\n",
        "encoder = OneHotEncoder()\n",
        "y_synthetic = encoder.fit_transform(y_synthetic).toarray()\n",
        "\n",
        "# Verify shapes\n",
        "print(f\"Shape of X_synthetic: {X_synthetic.shape}\")\n",
        "print(f\"Shape of y_synthetic: {y_synthetic.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from keras.models import load_model\n",
        "# from keras.callbacks import EarlyStopping\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# # Function to generate synthetic data\n",
        "# def generate_synthetic_data(model, num_samples=10000, num_timesteps=1, num_features=78):\n",
        "#     np.random.seed(42)\n",
        "#     noise = np.random.normal(size=(num_samples, num_timesteps, num_features))\n",
        "#     synthetic_data = model.predict(noise)\n",
        "#     return noise, synthetic_data\n",
        "\n",
        "# # Load LSTM model and generate synthetic data\n",
        "# lstm_model = load_model('/content/drive/MyDrive/lstm_best_model.h5')\n",
        "# num_samples = 10000\n",
        "# num_timesteps = 1  # Match the expected timesteps\n",
        "# num_features = 78  # Match the expected features\n",
        "# X_synthetic, y_synthetic = generate_synthetic_data(lstm_model, num_samples, num_timesteps, num_features)\n",
        "\n",
        "# # Ensure correct shapes and sizes\n",
        "# X_synthetic = X_synthetic.reshape(num_samples, num_features)  # Flatten for CSV saving\n",
        "# y_synthetic = y_synthetic.reshape(num_samples, -1)  # Ensure 2D shape for saving\n",
        "\n",
        "# # Save synthetic data\n",
        "# synthetic_data_path = '/content/drive/MyDrive/generated_data.csv'\n",
        "# pd.DataFrame(X_synthetic).to_csv(synthetic_data_path, index=False)\n",
        "# pd.DataFrame(y_synthetic).to_csv(synthetic_data_path.replace('.csv', '_labels.csv'), index=False)\n",
        "\n",
        "# print(f\"Synthetic data saved to {synthetic_data_path}\")\n",
        "\n",
        "# # Load the existing 6G model\n",
        "# model_6G = load_model('/content/drive/MyDrive/trained_6G_model_label.h5')\n",
        "\n",
        "# # Load synthetic data\n",
        "# X_synthetic = pd.read_csv(synthetic_data_path).values\n",
        "# y_synthetic = pd.read_csv(synthetic_data_path.replace('.csv', '_labels.csv')).values\n",
        "\n",
        "# # One-hot encode labels if necessary\n",
        "# encoder = OneHotEncoder()\n",
        "# y_synthetic = encoder.fit_transform(y_synthetic).toarray()\n",
        "\n",
        "# # Verify shapes\n",
        "# print(f\"Shape of X_synthetic: {X_synthetic.shape}\")\n",
        "# print(f\"Shape of y_synthetic: {y_synthetic.shape}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CKJDmFbay5WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from keras.models import load_model\n",
        "# from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# # Load synthetic data\n",
        "# X_synthetic = pd.read_csv('/content/drive/MyDrive/generated_data.csv').values\n",
        "# y_synthetic = pd.read_csv('/content/drive/MyDrive/generated_data_labels.csv').values\n",
        "\n",
        "# # Standardize the data\n",
        "# scaler = StandardScaler()\n",
        "# X_synthetic = scaler.fit_transform(X_synthetic)\n",
        "\n",
        "# # One-hot encode labels if necessary\n",
        "# encoder = OneHotEncoder(sparse_output=False)\n",
        "# y_synthetic = encoder.fit_transform(y_synthetic)\n",
        "\n",
        "# # Verify shapes\n",
        "# print(f\"Shape of X_synthetic: {X_synthetic.shape}\")\n",
        "# print(f\"Shape of y_synthetic: {y_synthetic.shape}\")\n",
        "\n",
        "# # Split synthetic data into training and validation sets\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_synthetic, y_synthetic, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Load the existing 6G model\n",
        "# model_6G = load_model('/content/drive/MyDrive/trained_6G_model_label.h5')\n",
        "\n",
        "# # Define batch size and number of epochs\n",
        "# batch_size = 32\n",
        "# epochs = 50  # Increase epochs for better accuracy but manage with early stopping\n",
        "\n",
        "# # Use EarlyStopping and ModelCheckpoint\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "# checkpoint = ModelCheckpoint('/content/drive/MyDrive/checkpoint_6G_model.h5', save_best_only=True, monitor='val_loss')\n",
        "\n",
        "# # Fine-tune the model incrementally to avoid crashes\n",
        "# steps_per_epoch = len(X_train) // batch_size\n",
        "\n",
        "# history = model_6G.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "#                        epochs=epochs, batch_size=batch_size,\n",
        "#                        steps_per_epoch=steps_per_epoch,\n",
        "#                        callbacks=[early_stopping, checkpoint])\n",
        "\n",
        "# # Load the best model saved by ModelCheckpoint\n",
        "# model_6G.load_weights('/content/drive/MyDrive/checkpoint_6G_model.h5')\n",
        "\n",
        "# # Save the fine-tuned model\n",
        "# fine_tuned_model_path = '/content/drive/MyDrive/retrained_finetuned_6G_model_label.h5'\n",
        "# model_6G.save(fine_tuned_model_path)\n",
        "\n",
        "# print(f\"Fine-tuned model saved to {fine_tuned_model_path}\")\n",
        "\n",
        "# # Evaluate and print metrics\n",
        "# y_pred = model_6G.predict(X_val, batch_size=batch_size)\n",
        "# y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "# y_true_classes = np.argmax(y_val, axis=1)\n",
        "\n",
        "# accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
        "# print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# print(\"Classification Report:\")\n",
        "# print(classification_report(y_true_classes, y_pred_classes))\n",
        "\n",
        "# print(\"Confusion Matrix:\")\n",
        "# print(confusion_matrix(y_true_classes, y_pred_classes))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Split synthetic data into training and validation sets\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_synthetic, y_synthetic, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Reduce batch size and epochs to prevent crashing\n",
        "# batch_size = 32  # Reduce batch size\n",
        "# epochs = 5  # Reduce number of epochs\n",
        "\n",
        "# # Fine-tune the model\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "# history = model_6G.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "#                        epochs=epochs, batch_size=batch_size, callbacks=[early_stopping])\n",
        "\n",
        "# # Save the fine-tuned model\n",
        "# fine_tuned_model_path = '/content/drive/MyDrive/retrained_finetuned_6G_model_label.h5'\n",
        "# model_6G.save(fine_tuned_model_path)\n",
        "\n",
        "# print(f\"Fine-tuned model saved to {fine_tuned_model_path}\")\n",
        "\n",
        "# # Evaluate and print metrics\n",
        "# y_pred = model_6G.predict(X_val, batch_size=batch_size)  # Predict with the same batch size\n",
        "# y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "# y_true_classes = np.argmax(y_val, axis=1)\n",
        "\n",
        "# accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
        "# print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# print(\"Classification Report:\")\n",
        "# print(classification_report(y_true_classes, y_pred_classes))\n",
        "\n",
        "# print(\"Confusion Matrix:\")\n",
        "# print(confusion_matrix(y_true_classes, y_pred_classes))"
      ],
      "metadata": {
        "id": "8o_OGfwTeE2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from keras.utils import Sequence\n",
        "\n",
        "# Custom data generator class\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, X_data, y_data, batch_size=16):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "        self.batch_size = batch_size\n",
        "        self.indices = np.arange(len(X_data))\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.X_data) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        X_batch = self.X_data[batch_indices]\n",
        "        y_batch = self.y_data[batch_indices]\n",
        "        return X_batch, y_batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "# Function to generate synthetic data\n",
        "def generate_synthetic_data(model, num_samples=10000, num_timesteps=1, num_features=78):\n",
        "    np.random.seed(42)\n",
        "    noise = np.random.normal(size=(num_samples, num_timesteps, num_features))\n",
        "    synthetic_data = model.predict(noise)\n",
        "    return noise, synthetic_data\n",
        "\n",
        "# Load LSTM model and generate synthetic data with correct shape\n",
        "lstm_model = load_model('/content/drive/MyDrive/lstm_best_model.h5')\n",
        "num_samples = 10000\n",
        "num_timesteps = 1  # Match the expected timesteps\n",
        "num_features = 78  # Use 78 to match the LSTM model's expected input shape\n",
        "X_synthetic, y_synthetic = generate_synthetic_data(lstm_model, num_samples, num_timesteps, num_features)\n",
        "\n",
        "# Ensure correct shapes and sizes\n",
        "X_synthetic = X_synthetic.reshape(num_samples, num_features)  # Flatten for CSV saving\n",
        "y_synthetic = y_synthetic.reshape(num_samples, -1)  # Ensure 2D shape for saving\n",
        "\n",
        "# Save synthetic data\n",
        "synthetic_data_path = '/content/drive/MyDrive/generated_data.csv'\n",
        "pd.DataFrame(X_synthetic).to_csv(synthetic_data_path, index=False)\n",
        "pd.DataFrame(y_synthetic).to_csv(synthetic_data_path.replace('.csv', '_labels.csv'), index=False)\n",
        "\n",
        "print(f\"Synthetic data saved to {synthetic_data_path}\")\n",
        "\n",
        "# Load synthetic data\n",
        "X_synthetic = pd.read_csv(synthetic_data_path).values\n",
        "y_synthetic = pd.read_csv(synthetic_data_path.replace('.csv', '_labels.csv')).values\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_synthetic = scaler.fit_transform(X_synthetic)\n",
        "\n",
        "# One-hot encode labels if necessary\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_synthetic = encoder.fit_transform(y_synthetic)\n",
        "\n",
        "# Verify shapes\n",
        "print(f\"Shape of X_synthetic: {X_synthetic.shape}\")\n",
        "print(f\"Shape of y_synthetic: {y_synthetic.shape}\")"
      ],
      "metadata": {
        "id": "Yh3psWfruaAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0Rz-FmBEugyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split synthetic data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_synthetic, y_synthetic, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check input shape for the model\n",
        "input_shape = model_6G.input_shape[1]\n",
        "if X_train.shape[1] != input_shape:\n",
        "    raise ValueError(f\"Expected input shape {input_shape} but got {X_train.shape[1]}\")\n",
        "\n",
        "# Define batch size and number of epochs\n",
        "batch_size = 16  # Reduced batch size\n",
        "epochs = 20  # Reasonable number of epochs\n",
        "\n",
        "# Use EarlyStopping and ModelCheckpoint\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('/content/drive/MyDrive/checkpoint_6G_model.h5', save_best_only=True, monitor='val_loss')\n",
        "\n",
        "# Create data generators\n",
        "train_generator = DataGenerator(X_train, y_train, batch_size=batch_size)\n",
        "val_generator = DataGenerator(X_val, y_val, batch_size=batch_size)\n",
        "\n",
        "# Fine-tune the model using the data generators\n",
        "history = model_6G.fit(train_generator,\n",
        "                       validation_data=val_generator,\n",
        "                       epochs=epochs,\n",
        "                       callbacks=[early_stopping, checkpoint])\n",
        "\n",
        "# Load the best model saved by ModelCheckpoint\n",
        "model_6G.load_weights('/content/drive/MyDrive/checkpoint_6G_model.h5')\n",
        "\n",
        "# Save the fine-tuned model\n",
        "fine_tuned_model_path = '/content/drive/MyDrive/retrained_finetuned_6G_model_label.h5'\n",
        "model_6G.save(fine_tuned_model_path)\n",
        "\n",
        "print(f\"Fine-tuned model saved to {fine_tuned_model_path}\")\n",
        "\n",
        "# Evaluate and print metrics\n",
        "y_pred = model_6G.predict(val_generator)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_val, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true_classes, y_pred_classes))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_true_classes, y_pred_classes))"
      ],
      "metadata": {
        "id": "uBbKTWQ4ufY6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyP5+ipyLRy2ZlXccl0T0fXk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}